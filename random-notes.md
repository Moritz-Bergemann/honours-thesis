# Chapter 2
While the domain transferability of general vision transformers and state-of-the art performance of segmentation transformers like SegFormer had both been established, there had been no exploration into combining these strengths as of early 2022.
The intuition here was that, just as vision transformers outperformed existing domain transfer approaches even with naive exmaples, there was a real possibility that a similar boost of performance could be achieved by segmentation transformers.

However, there are some differences between the application of domain adaptation in this context compared to in simpler computer vision approaches. For example, the fact that segmentation models output a number of predictions has required adaptations classificiation approaches where only one output is produced [CONSIDER EXPANDING ON THIS].

An intuitive approach to this problem would be to first apply a model like segformer to domain adaptation, first evaluating its performance in a naive setting (train on source, evaluate on target) and then using an approach tailored to domain adaptation. To avoid having to interact with many of the complexities brought on by moving from a convolutional to transformer architecture, a largely architecture-independent approach proven for segmentation, such as self-training [CITE] may be used. While this was the initial plan for this chapter of the thesis, it was also unfortunately the precise subject of the [MONTH] 2022 paper DAFormer by [AUTHOR] [CITE]. [SPECIFIC DESCRIPTION OF WHAT THEY DID AND HOW THEY DID IT, PLUS THE BONUS STUFF THEY DID LIKE IMAGENET FEATURE DISTANCE]. As I sought to explore novel concepts and approaches, I instead shifted focus onto the other popular means of performing domain adaptation - domain-adversarial training. Domain adversarial approaches were not explored in DAFormer due to the recent success of self-training in other research and the difficulty of implementing domain adversarial methods in segmentation transformers. The challenge comes primarily from the large changes that must be made to the training pipeline (adding a peripheral network with a separate training goal).

My final training pipeline was built on top of the DAFormer repository (itself built on the MMSegmentation framework). This allowed us to take advantage of the general framework changes necessary for domain adaptation, namely allowing for the input and preprocessing of multiple datasets.