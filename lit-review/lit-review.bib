
@article{zheng_rethinking_2021,
	title = {Rethinking {Semantic} {Segmentation} from a {Sequence}-to-{Sequence} {Perspective} with {Transformers}},
	url = {http://arxiv.org/abs/2012.15840},
	abstract = {Most recent semantic segmentation methods adopt a fully-convolutional network (FCN) with an encoder-decoder architecture. The encoder progressively reduces the spatial resolution and learns more abstract/semantic visual concepts with larger receptive fields. Since context modeling is critical for segmentation, the latest efforts have been focused on increasing the receptive field, through either dilated/atrous convolutions or inserting attention modules. However, the encoder-decoder based FCN architecture remains unchanged. In this paper, we aim to provide an alternative perspective by treating semantic segmentation as a sequence-to-sequence prediction task. Specifically, we deploy a pure transformer (ie, without convolution and resolution reduction) to encode an image as a sequence of patches. With the global context modeled in every layer of the transformer, this encoder can be combined with a simple decoder to provide a powerful segmentation model, termed SEgmentation TRansformer (SETR). Extensive experiments show that SETR achieves new state of the art on ADE20K (50.28\% mIoU), Pascal Context (55.83\% mIoU) and competitive results on Cityscapes. Particularly, we achieve the first position in the highly competitive ADE20K test server leaderboard on the day of submission.},
	urldate = {2022-04-04},
	journal = {arXiv:2012.15840 [cs]},
	author = {Zheng, Sixiao and Lu, Jiachen and Zhao, Hengshuang and Zhu, Xiatian and Luo, Zekun and Wang, Yabiao and Fu, Yanwei and Feng, Jianfeng and Xiang, Tao and Torr, Philip H. S. and Zhang, Li},
	month = jul,
	year = {2021},
	note = {arXiv: 2012.15840},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR 2021. Project page at https://fudan-zvg.github.io/SETR/},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\D5GM2RSU\\Zheng et al. - 2021 - Rethinking Semantic Segmentation from a Sequence-t.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\RQ97KMN9\\2012.html:text/html},
}

@article{xie_segformer_2021,
	title = {{SegFormer}: {Simple} and {Efficient} {Design} for {Semantic} {Segmentation} with {Transformers}},
	shorttitle = {{SegFormer}},
	url = {http://arxiv.org/abs/2105.15203},
	abstract = {We present SegFormer, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with lightweight multilayer perception (MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a novel hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding, thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution differs from training. 2) SegFormer avoids complex decoders. The proposed MLP decoder aggregates information from different layers, and thus combining both local attention and global attention to render powerful representations. We show that this simple and lightweight design is the key to efficient segmentation on Transformers. We scale our approach up to obtain a series of models from SegFormer-B0 to SegFormer-B5, reaching significantly better performance and efficiency than previous counterparts. For example, SegFormer-B4 achieves 50.3\% mIoU on ADE20K with 64M parameters, being 5x smaller and 2.2\% better than the previous best method. Our best model, SegFormer-B5, achieves 84.0\% mIoU on Cityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C. Code will be released at: github.com/NVlabs/SegFormer.},
	urldate = {2022-03-28},
	journal = {arXiv:2105.15203 [cs]},
	author = {Xie, Enze and Wang, Wenhai and Yu, Zhiding and Anandkumar, Anima and Alvarez, Jose M. and Luo, Ping},
	month = oct,
	year = {2021},
	note = {arXiv: 2105.15203},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: Accepted by NeurIPS 2021},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\FGHU2AKL\\Xie et al. - 2021 - SegFormer Simple and Efficient Design for Semanti.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\UIW97F3Q\\2105.html:text/html},
}

@article{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2022-03-28},
	journal = {arXiv:1706.03762 [cs]},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv: 1706.03762},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: 15 pages, 5 figures},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\RA7YVMTY\\Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\ZISK5JH3\\1706.html:text/html},
}

@article{chun_road_2019,
	title = {Road {Surface} {Damage} {Detection} {Using} {Fully} {Convolutional} {Neural} {Networks} and {Semi}-{Supervised} {Learning}},
	volume = {19},
	issn = {1424-8220},
	doi = {10.3390/s19245501},
	abstract = {The various defects that occur on asphalt pavement are a direct cause car accidents, and countermeasures are required because they cause significantly dangerous situations. In this paper, we propose fully convolutional neural networks (CNN)-based road surface damage detection with semi-supervised learning. First, the training DB is collected through the camera installed in the vehicle while driving on the road. Moreover, the CNN model is trained in the form of a semantic segmentation using the deep convolutional autoencoder. Here, we augmented the training dataset depending on brightness, and finally generated a total of 40,536 training images. Furthermore, the CNN model is updated by using the pseudo-labeled images from the semi-supervised learning methods for improving the performance of road surface damage detection technique. To demonstrate the effectiveness of the proposed method, 450 evaluation datasets were created to verify the performance of the proposed road surface damage detection, and four experts evaluated each image. As a result, it is confirmed that the proposed method can properly segment the road surface damages.},
	language = {eng},
	number = {24},
	journal = {Sensors (Basel, Switzerland)},
	author = {Chun, Chanjun and Ryu, Seung-Ki},
	month = dec,
	year = {2019},
	pmid = {31842513},
	pmcid = {PMC6961057},
	keywords = {autoencoder, convolutional neural network, road surface damage, semantic segmentation, semi-supervised learning},
	pages = {E5501},
	file = {Full Text:C\:\\Users\\morit\\Zotero\\storage\\PEFNGLH2\\Chun and Ryu - 2019 - Road Surface Damage Detection Using Fully Convolut.pdf:application/pdf},
}

@inproceedings{nayyeri_multi-resolution_2021,
	title = {Multi-{Resolution} {ResNet} for {Road} and {Bridge} {Crack} {Detection}},
	doi = {10.1109/DICTA52665.2021.9647398},
	abstract = {In this work, we present a novel ResNet-like approach called MR-CrackNet to detect and localise infrastructural cracks of different sizes. In a series of consecutive cycles, low-level feature maps such as crack edges and boundaries are extracted and combined with multi-resolution high-level feature maps such as crack regions. Two separate streams are designed to carry these feature maps after each cycle: low-level stream carries crack feature maps in full resolution, and high-level stream send crack features through an encoder-decoder network to create feature maps in different low resolutions. At each cycle, two feature maps are combined, processed and sent back through two streams for the next cycle. This model is trained and evaluated on a new crack dataset of 2,532 images. Quantitative and qualitative results show that MR-CrackNet outperforms the baseline models with a clear margin and it is able to extract significant crack features and achieve highly accurate crack detection.},
	booktitle = {2021 {Digital} {Image} {Computing}: {Techniques} and {Applications} ({DICTA})},
	author = {Nayyeri, Fereshteh and Zhou, Jun},
	month = nov,
	year = {2021},
	keywords = {Bridges, Computational modeling, crack dataset, crack detection, Digital images, Image edge detection, Image resolution, localisation, multi resolution, ResNet, Roads, Training},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\morit\\Zotero\\storage\\DGV2FG7J\\9647398.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\morit\\Zotero\\storage\\TSFZAM9K\\Nayyeri and Zhou - 2021 - Multi-Resolution ResNet for Road and Bridge Crack .pdf:application/pdf},
}

@article{hamishebahar_comprehensive_2022,
	title = {A {Comprehensive} {Review} of {Deep} {Learning}-{Based} {Crack} {Detection} {Approaches}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/12/3/1374},
	doi = {10.3390/app12031374},
	abstract = {The application of deep architectures inspired by the fields of artificial intelligence and computer vision has made a significant impact on the task of crack detection. As the number of studies being published in this field is growing fast, it is important to categorize the studies at deeper levels. In this paper, a comprehensive literature review of deep learning-based crack detection studies and the contributions they have made to the field is presented. The studies are categorised according to the computer vision aspect and at deeper levels to facilitate exploring the studies that utilised similar approaches to address the crack detection problem. Moreover, the authors perform a comparison between the studies which use the same publicly available data sets, in order to find the most promising crack detection approaches. Critical future directions for research are proposed, based on these reviewed studies as well as on trends and developments in areas similar to the crack detection area.},
	language = {en},
	number = {3},
	urldate = {2022-03-21},
	journal = {Applied Sciences},
	author = {Hamishebahar, Younes and Guan, Hong and So, Stephen and Jo, Jun},
	month = jan,
	year = {2022},
	note = {Number: 3
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {semantic segmentation, crack detection, deep learning, image classification, object recognition, structural health monitoring},
	pages = {1374},
	file = {Full Text PDF:C\:\\Users\\morit\\Zotero\\storage\\LTRRPFZ3\\Hamishebahar et al. - 2022 - A Comprehensive Review of Deep Learning-Based Crac.pdf:application/pdf;Snapshot:C\:\\Users\\morit\\Zotero\\storage\\NZLNKEEU\\htm.html:text/html},
}

@inproceedings{sener_learning_2016,
	title = {Learning {Transferrable} {Representations} for {Unsupervised} {Domain} {Adaptation}},
	volume = {29},
	url = {https://papers.nips.cc/paper/2016/hash/b59c67bf196a4758191e42f76670ceba-Abstract.html},
	abstract = {Supervised learning with large scale labelled datasets and deep layered models has caused a paradigm shift in diverse areas in learning and recognition. However, this approach still suffers from generalization issues under the presence of a domain shift between the training and the test data distribution. Since unsupervised domain adaptation algorithms directly address this domain shift problem between a labelled source dataset and an unlabelled target dataset, recent papers have shown promising results by fine-tuning the networks with domain adaptation loss functions which try to align the mismatch between the training and testing data distributions.  Nevertheless, these recent deep learning based domain adaptation approaches still suffer from issues such as high sensitivity to the gradient reversal hyperparameters and overfitting during the fine-tuning stage. In this paper, we propose a unified deep learning framework where the representation, cross domain transformation, and target label inference are all jointly optimized in an end-to-end fashion for unsupervised domain adaptation. Our experiments show that the proposed method significantly outperforms state-of-the-art algorithms in both object recognition and digit classification experiments by a large margin. We will make our learned models as well as the source code available immediately upon acceptance.},
	urldate = {2022-04-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Sener, Ozan and Song, Hyun Oh and Saxena, Ashutosh and Savarese, Silvio},
	year = {2016},
	file = {Full Text PDF:C\:\\Users\\morit\\Zotero\\storage\\25QWWYIL\\Sener et al. - 2016 - Learning Transferrable Representations for Unsuper.pdf:application/pdf},
}

@misc{noauthor_papers_nodate,
	title = {Papers with {Code} - {TopFormer}: {Token} {Pyramid} {Transformer} for {Mobile} {Semantic} {Segmentation}},
	shorttitle = {Papers with {Code} - {TopFormer}},
	url = {https://paperswithcode.com/paper/topformer-token-pyramid-transformer-for},
	abstract = {Implemented in one code library.},
	language = {en},
	urldate = {2022-04-17},
	file = {Snapshot:C\:\\Users\\morit\\Zotero\\storage\\EW96LJY2\\topformer-token-pyramid-transformer-for.html:text/html},
}

@article{borse_inverseform_2021,
	title = {{InverseForm}: {A} {Loss} {Function} for {Structured} {Boundary}-{Aware} {Segmentation}},
	shorttitle = {{InverseForm}},
	url = {http://arxiv.org/abs/2104.02745},
	abstract = {We present a novel boundary-aware loss term for semantic segmentation using an inverse-transformation network, which efficiently learns the degree of parametric transformations between estimated and target boundaries. This plug-in loss term complements the cross-entropy loss in capturing boundary transformations and allows consistent and significant performance improvement on segmentation backbone models without increasing their size and computational complexity. We analyze the quantitative and qualitative effects of our loss function on three indoor and outdoor segmentation benchmarks, including Cityscapes, NYU-Depth-v2, and PASCAL, integrating it into the training phase of several backbone networks in both single-task and multi-task settings. Our extensive experiments show that the proposed method consistently outperforms baselines, and even sets the new state-of-the-art on two datasets.},
	urldate = {2022-04-18},
	journal = {arXiv:2104.02745 [cs]},
	author = {Borse, Shubhankar and Wang, Ying and Zhang, Yizhe and Porikli, Fatih},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.02745},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: Accepted to CVPR 2021 as an oral presentation},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\WJ7PIVHI\\Borse et al. - 2021 - InverseForm A Loss Function for Structured Bounda.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\5VT5GPH8\\2104.html:text/html},
}

@incollection{ferrari_unsupervised_2018,
	address = {Cham},
	title = {Unsupervised {Domain} {Adaptation} for {Semantic} {Segmentation} via {Class}-{Balanced} {Self}-training},
	volume = {11207},
	isbn = {978-3-030-01218-2 978-3-030-01219-9},
	url = {http://link.springer.com/10.1007/978-3-030-01219-9_18},
	abstract = {Recent deep networks achieved state of the art performance on a variety of semantic segmentation tasks. Despite such progress, these models often face challenges in real world “wild tasks” where large diﬀerence between labeled training/source data and unseen test/target data exists. In particular, such diﬀerence is often referred to as “domain gap”, and could cause signiﬁcantly decreased performance which cannot be easily remedied by further increasing the representation power. Unsupervised domain adaptation (UDA) seeks to overcome such problem without target domain labels. In this paper, we propose a novel UDA framework based on an iterative self-training (ST) procedure, where the problem is formulated as latent variable loss minimization, and can be solved by alternatively generating pseudo labels on target data and re-training the model with these labels. On top of ST, we also propose a novel classbalanced self-training (CBST) framework to avoid the gradual dominance of large classes on pseudo-label generation, and introduce spatial priors to reﬁne generated labels. Comprehensive experiments show that the proposed methods achieve state of the art semantic segmentation performance under multiple major UDA settings.},
	language = {en},
	urldate = {2022-04-18},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Zou, Yang and Yu, Zhiding and Vijaya Kumar, B. V. K. and Wang, Jinsong},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	doi = {10.1007/978-3-030-01219-9_18},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {297--313},
	file = {Zou et al. - 2018 - Unsupervised Domain Adaptation for Semantic Segmen.pdf:C\:\\Users\\morit\\Zotero\\storage\\X2FECEBH\\Zou et al. - 2018 - Unsupervised Domain Adaptation for Semantic Segmen.pdf:application/pdf},
}

@article{liang_domain_2021,
	title = {Domain {Adaptation} with {Auxiliary} {Target} {Domain}-{Oriented} {Classifier}},
	url = {http://arxiv.org/abs/2007.04171},
	abstract = {Domain adaptation (DA) aims to transfer knowledge from a label-rich but heterogeneous domain to a label-scare domain, which alleviates the labeling efforts and attracts considerable attention. Different from previous methods focusing on learning domain-invariant feature representations, some recent methods present generic semi-supervised learning (SSL) techniques and directly apply them to DA tasks, even achieving competitive performance. One of the most popular SSL techniques is pseudo-labeling that assigns pseudo labels for each unlabeled data via the classifier trained by labeled data. However, it ignores the distribution shift in DA problems and is inevitably biased to source data. To address this issue, we propose a new pseudo-labeling framework called Auxiliary Target Domain-Oriented Classifier (ATDOC). ATDOC alleviates the classifier bias by introducing an auxiliary classifier for target data only, to improve the quality of pseudo labels. Specifically, we employ the memory mechanism and develop two types of non-parametric classifiers, i.e. the nearest centroid classifier and neighborhood aggregation, without introducing any additional network parameters. Despite its simplicity in a pseudo classification objective, ATDOC with neighborhood aggregation significantly outperforms domain alignment techniques and prior SSL techniques on a large variety of DA benchmarks and even scare-labeled SSL tasks.},
	urldate = {2022-04-25},
	journal = {arXiv:2007.04171 [cs]},
	author = {Liang, Jian and Hu, Dapeng and Feng, Jiashi},
	month = dec,
	year = {2021},
	note = {arXiv: 2007.04171},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: Fix typos after CVPR 2021. Code is available at https://github.com/tim-learn/ATDOC},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\6RGIFSRI\\Liang et al. - 2021 - Domain Adaptation with Auxiliary Target Domain-Ori.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\F6VRUX53\\2007.html:text/html},
}

@techreport{li_dispensed_2021,
	title = {Dispensed {Transformer} {Network} for {Unsupervised} {Domain} {Adaptation}},
	url = {http://arxiv.org/abs/2110.14944},
	abstract = {Accurate segmentation is a crucial step in medical image analysis and applying supervised machine learning to segment the organs or lesions has been substantiated effective. However, it is costly to perform data annotation that provides ground truth labels for training the supervised algorithms, and the high variance of data that comes from different domains tends to severely degrade system performance over cross-site or cross-modality datasets. To mitigate this problem, a novel unsupervised domain adaptation (UDA) method named dispensed Transformer network (DTNet) is introduced in this paper. Our novel DTNet contains three modules. First, a dispensed residual transformer block is designed, which realizes global attention by dispensed interleaving operation and deals with the excessive computational cost and GPU memory usage of the Transformer. Second, a multi-scale consistency regularization is proposed to alleviate the loss of details in the low-resolution output for better feature alignment. Finally, a feature ranking discriminator is introduced to automatically assign different weights to domain-gap features to lessen the feature distribution distance, reducing the performance shift of two domains. The proposed method is evaluated on large fluorescein angiography (FA) retinal nonperfusion (RNP) cross-site dataset with 676 images and a wide used cross-modality dataset from the MM-WHS challenge. Extensive results demonstrate that our proposed network achieves the best performance in comparison with several state-of-the-art techniques.},
	number = {arXiv:2110.14944},
	urldate = {2022-05-17},
	institution = {arXiv},
	author = {Li, Yunxiang and Li, Jingxiong and Dan, Ruilong and Wang, Shuai and Jin, Kai and Zeng, Guodong and Wang, Jun and Pan, Xiangji and Zhang, Qianni and Zhou, Huiyu and Jin, Qun and Wang, Li and Wang, Yaqi},
	month = oct,
	year = {2021},
	note = {arXiv:2110.14944 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	annote = {Comment: 10 pages},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\I8GV85PS\\Li et al. - 2021 - Dispensed Transformer Network for Unsupervised Dom.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\6DEC6UTT\\2110.html:text/html},
}

@techreport{mehta_mobilevit_2022,
	title = {{MobileViT}: {Light}-weight, {General}-purpose, and {Mobile}-friendly {Vision} {Transformer}},
	shorttitle = {{MobileViT}},
	url = {http://arxiv.org/abs/2110.02178},
	abstract = {Light-weight convolutional neural networks (CNNs) are the de-facto for mobile vision tasks. Their spatial inductive biases allow them to learn representations with fewer parameters across different vision tasks. However, these networks are spatially local. To learn global representations, self-attention-based vision trans-formers (ViTs) have been adopted. Unlike CNNs, ViTs are heavy-weight. In this paper, we ask the following question: is it possible to combine the strengths of CNNs and ViTs to build a light-weight and low latency network for mobile vision tasks? Towards this end, we introduce MobileViT, a light-weight and general-purpose vision transformer for mobile devices. MobileViT presents a different perspective for the global processing of information with transformers, i.e., transformers as convolutions. Our results show that MobileViT significantly outperforms CNN- and ViT-based networks across different tasks and datasets. On the ImageNet-1k dataset, MobileViT achieves top-1 accuracy of 78.4\% with about 6 million parameters, which is 3.2\% and 6.2\% more accurate than MobileNetv3 (CNN-based) and DeIT (ViT-based) for a similar number of parameters. On the MS-COCO object detection task, MobileViT is 5.7\% more accurate than MobileNetv3 for a similar number of parameters. Our source code is open-source and available at: https://github.com/apple/ml-cvnets},
	number = {arXiv:2110.02178},
	urldate = {2022-05-17},
	institution = {arXiv},
	author = {Mehta, Sachin and Rastegari, Mohammad},
	month = mar,
	year = {2022},
	doi = {10.48550/arXiv.2110.02178},
	note = {arXiv:2110.02178 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: Accepted at ICLR'22},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\7AXV588K\\Mehta and Rastegari - 2022 - MobileViT Light-weight, General-purpose, and Mobi.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\C9DY9JU7\\2110.html:text/html},
}

@misc{noauthor_210805988_nodate,
	title = {[2108.05988] {TVT}: {Transferable} {Vision} {Transformer} for {Unsupervised} {Domain} {Adaptation}},
	url = {https://arxiv.org/abs/2108.05988},
	urldate = {2022-05-03},
	file = {[2108.05988] TVT\: Transferable Vision Transformer for Unsupervised Domain Adaptation:C\:\\Users\\morit\\Zotero\\storage\\48D92DPE\\2108.html:text/html},
}

@techreport{long_fully_2015,
	title = {Fully {Convolutional} {Networks} for {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/1411.4038},
	abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
	number = {arXiv:1411.4038},
	urldate = {2022-05-24},
	institution = {arXiv},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	month = mar,
	year = {2015},
	doi = {10.48550/arXiv.1411.4038},
	note = {arXiv:1411.4038 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: to appear in CVPR (2015)},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\UYH56QXN\\Long et al. - 2015 - Fully Convolutional Networks for Semantic Segmenta.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\4B238W9H\\1411.html:text/html},
}

@techreport{chen_deeplab_2017,
	title = {{DeepLab}: {Semantic} {Image} {Segmentation} with {Deep} {Convolutional} {Nets}, {Atrous} {Convolution}, and {Fully} {Connected} {CRFs}},
	shorttitle = {{DeepLab}},
	url = {http://arxiv.org/abs/1606.00915},
	abstract = {In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or 'atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed "DeepLab" system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7\% mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
	number = {arXiv:1606.00915},
	urldate = {2022-05-24},
	institution = {arXiv},
	author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
	month = may,
	year = {2017},
	doi = {10.48550/arXiv.1606.00915},
	note = {arXiv:1606.00915 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted by TPAMI},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\TK3KFD2J\\Chen et al. - 2017 - DeepLab Semantic Image Segmentation with Deep Con.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\E7RR4795\\1606.html:text/html},
}

@techreport{minaee_image_2020,
	title = {Image {Segmentation} {Using} {Deep} {Learning}: {A} {Survey}},
	shorttitle = {Image {Segmentation} {Using} {Deep} {Learning}},
	url = {http://arxiv.org/abs/2001.05566},
	abstract = {Image segmentation is a key topic in image processing and computer vision with applications such as scene understanding, medical image analysis, robotic perception, video surveillance, augmented reality, and image compression, among many others. Various algorithms for image segmentation have been developed in the literature. Recently, due to the success of deep learning models in a wide range of vision applications, there has been a substantial amount of works aimed at developing image segmentation approaches using deep learning models. In this survey, we provide a comprehensive review of the literature at the time of this writing, covering a broad spectrum of pioneering works for semantic and instance-level segmentation, including fully convolutional pixel-labeling networks, encoder-decoder architectures, multi-scale and pyramid based approaches, recurrent networks, visual attention models, and generative models in adversarial settings. We investigate the similarity, strengths and challenges of these deep learning models, examine the most widely used datasets, report performances, and discuss promising future research directions in this area.},
	number = {arXiv:2001.05566},
	urldate = {2022-05-27},
	institution = {arXiv},
	author = {Minaee, Shervin and Boykov, Yuri and Porikli, Fatih and Plaza, Antonio and Kehtarnavaz, Nasser and Terzopoulos, Demetri},
	month = nov,
	year = {2020},
	note = {arXiv:2001.05566 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\WDYA8C8F\\Minaee et al. - 2020 - Image Segmentation Using Deep Learning A Survey.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\9V23FISH\\2001.html:text/html},
}

@techreport{xu_cdtrans_2021,
	title = {{CDTrans}: {Cross}-domain {Transformer} for {Unsupervised} {Domain} {Adaptation}},
	shorttitle = {{CDTrans}},
	url = {http://arxiv.org/abs/2109.06165},
	abstract = {Unsupervised domain adaptation (UDA) aims to transfer knowledge learned from a labeled source domain to a different unlabeled target domain. Most existing UDA methods focus on learning domain-invariant feature representation, either from the domain level or category level, using convolution neural networks (CNNs)-based frameworks. One fundamental problem for the category level based UDA is the production of pseudo labels for samples in target domain, which are usually too noisy for accurate domain alignment, inevitably compromising the UDA performance. With the success of Transformer in various tasks, we find that the cross-attention in Transformer is robust to the noisy input pairs for better feature alignment, thus in this paper Transformer is adopted for the challenging UDA task. Specifically, to generate accurate input pairs, we design a two-way center-aware labeling algorithm to produce pseudo labels for target samples. Along with the pseudo labels, a weight-sharing triple-branch transformer framework is proposed to apply self-attention and cross-attention for source/target feature learning and source-target domain alignment, respectively. Such design explicitly enforces the framework to learn discriminative domain-specific and domain-invariant representations simultaneously. The proposed method is dubbed CDTrans (cross-domain transformer), and it provides one of the first attempts to solve UDA tasks with a pure transformer solution. Experiments show that our proposed method achieves the best performance on public UDA datasets, e.g. VisDA-2017 and DomainNet. Code and models are available at https://github.com/CDTrans/CDTrans.},
	number = {arXiv:2109.06165},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Xu, Tongkun and Chen, Weihua and Wang, Pichao and Wang, Fan and Li, Hao and Jin, Rong},
	month = sep,
	year = {2021},
	doi = {10.48550/arXiv.2109.06165},
	note = {arXiv:2109.06165 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\HNBPXMLS\\Xu et al. - 2022 - CDTrans Cross-domain Transformer for Unsupervised.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\DQGWCHK8\\2109.html:text/html},
}

@techreport{wilson_survey_2020,
	title = {A {Survey} of {Unsupervised} {Deep} {Domain} {Adaptation}},
	url = {http://arxiv.org/abs/1812.02849},
	abstract = {Deep learning has produced state-of-the-art results for a variety of tasks. While such approaches for supervised learning have performed well, they assume that training and testing data are drawn from the same distribution, which may not always be the case. As a complement to this challenge, single-source unsupervised domain adaptation can handle situations where a network is trained on labeled data from a source domain and unlabeled data from a related but different target domain with the goal of performing well at test-time on the target domain. Many single-source and typically homogeneous unsupervised deep domain adaptation approaches have thus been developed, combining the powerful, hierarchical representations from deep learning with domain adaptation to reduce reliance on potentially-costly target data labels. This survey will compare these approaches by examining alternative methods, the unique and common elements, results, and theoretical insights. We follow this with a look at application areas and open research directions.},
	number = {arXiv:1812.02849},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Wilson, Garrett and Cook, Diane J.},
	month = feb,
	year = {2020},
	doi = {10.48550/arXiv.1812.02849},
	note = {arXiv:1812.02849 [cs, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\D8TAF3XJ\\Wilson and Cook - 2020 - A Survey of Unsupervised Deep Domain Adaptation.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\5TZERQK6\\1812.html:text/html},
}

@techreport{wang_domain_2022,
	title = {Domain {Adaptation} via {Bidirectional} {Cross}-{Attention} {Transformer}},
	url = {http://arxiv.org/abs/2201.05887},
	abstract = {Domain Adaptation (DA) aims to leverage the knowledge learned from a source domain with ample labeled data to a target domain with unlabeled data only. Most existing studies on DA contribute to learning domain-invariant feature representations for both domains by minimizing the domain gap based on convolution-based neural networks. Recently, vision transformers significantly improved performance in multiple vision tasks. Built on vision transformers, in this paper we propose a Bidirectional Cross-Attention Transformer (BCAT) for DA with the aim to improve the performance. In the proposed BCAT, the attention mechanism can extract implicit source and target mix-up feature representations to narrow the domain discrepancy. Specifically, in BCAT, we design a weight-sharing quadruple-branch transformer with a bidirectional cross-attention mechanism to learn domain-invariant feature representations. Extensive experiments demonstrate that the proposed BCAT model achieves superior performance on four benchmark datasets over existing state-of-the-art DA methods that are based on convolutions or transformers.},
	number = {arXiv:2201.05887},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Wang, Xiyu and Guo, Pengxin and Zhang, Yu},
	month = jan,
	year = {2022},
	doi = {10.48550/arXiv.2201.05887},
	note = {arXiv:2201.05887 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\STYLRCZG\\Wang et al. - 2022 - Domain Adaptation via Bidirectional Cross-Attentio.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\TC5UWRST\\2201.html:text/html},
}

@techreport{farahani_brief_2020,
	title = {A {Brief} {Review} of {Domain} {Adaptation}},
	url = {http://arxiv.org/abs/2010.03978},
	abstract = {Classical machine learning assumes that the training and test sets come from the same distributions. Therefore, a model learned from the labeled training data is expected to perform well on the test data. However, This assumption may not always hold in real-world applications where the training and the test data fall from different distributions, due to many factors, e.g., collecting the training and test sets from different sources, or having an out-dated training set due to the change of data over time. In this case, there would be a discrepancy across domain distributions, and naively applying the trained model on the new dataset may cause degradation in the performance. Domain adaptation is a sub-field within machine learning that aims to cope with these types of problems by aligning the disparity between domains such that the trained model can be generalized into the domain of interest. This paper focuses on unsupervised domain adaptation, where the labels are only available in the source domain. It addresses the categorization of domain adaptation from different viewpoints. Besides, It presents some successful shallow and deep domain adaptation approaches that aim to deal with domain adaptation problems.},
	number = {arXiv:2010.03978},
	urldate = {2022-06-05},
	institution = {arXiv},
	author = {Farahani, Abolfazl and Voghoei, Sahar and Rasheed, Khaled and Arabnia, Hamid R.},
	month = oct,
	year = {2020},
	doi = {10.48550/arXiv.2010.03978},
	note = {arXiv:2010.03978 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\BM98UAEI\\Farahani et al. - 2020 - A Brief Review of Domain Adaptation.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\BYIRZSUJ\\2010.html:text/html},
}

@techreport{cordts_cityscapes_2016,
	title = {The {Cityscapes} {Dataset} for {Semantic} {Urban} {Scene} {Understanding}},
	url = {http://arxiv.org/abs/1604.01685},
	abstract = {Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations; 20000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.},
	number = {arXiv:1604.01685},
	urldate = {2022-06-05},
	institution = {arXiv},
	author = {Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
	month = apr,
	year = {2016},
	doi = {10.48550/arXiv.1604.01685},
	note = {arXiv:1604.01685 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Includes supplemental material},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\T9YT7PA9\\Cordts et al. - 2016 - The Cityscapes Dataset for Semantic Urban Scene Un.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\7LI9RM37\\1604.html:text/html},
}

@article{pan_survey_2010,
	title = {A {Survey} on {Transfer} {Learning}},
	volume = {22},
	issn = {1558-2191},
	doi = {10.1109/TKDE.2009.191},
	abstract = {A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research.},
	number = {10},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Pan, Sinno Jialin and Yang, Qiang},
	month = oct,
	year = {2010},
	note = {Conference Name: IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Machine learning, Data mining, data mining., Knowledge engineering, Knowledge transfer, Labeling, Learning systems, machine learning, Machine learning algorithms, Space technology, survey, Testing, Training data, Transfer learning},
	pages = {1345--1359},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\morit\\Zotero\\storage\\HLV9BCRD\\5288526.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\morit\\Zotero\\storage\\C5DJQ84Q\\Pan and Yang - 2010 - A Survey on Transfer Learning.pdf:application/pdf},
}

@techreport{chen_attention_2016,
	title = {Attention to {Scale}: {Scale}-aware {Semantic} {Image} {Segmentation}},
	shorttitle = {Attention to {Scale}},
	url = {http://arxiv.org/abs/1511.03339},
	abstract = {Incorporating multi-scale features in fully convolutional neural networks (FCNs) has been a key element to achieving state-of-the-art performance on semantic image segmentation. One common way to extract multi-scale features is to feed multiple resized input images to a shared deep network and then merge the resulting features for pixelwise classification. In this work, we propose an attention mechanism that learns to softly weight the multi-scale features at each pixel location. We adapt a state-of-the-art semantic image segmentation model, which we jointly train with multi-scale input images and the attention model. The proposed attention model not only outperforms average- and max-pooling, but allows us to diagnostically visualize the importance of features at different positions and scales. Moreover, we show that adding extra supervision to the output at each scale is essential to achieving excellent performance when merging multi-scale features. We demonstrate the effectiveness of our model with extensive experiments on three challenging datasets, including PASCAL-Person-Part, PASCAL VOC 2012 and a subset of MS-COCO 2014.},
	number = {arXiv:1511.03339},
	urldate = {2022-06-04},
	institution = {arXiv},
	author = {Chen, Liang-Chieh and Yang, Yi and Wang, Jiang and Xu, Wei and Yuille, Alan L.},
	month = jun,
	year = {2016},
	note = {arXiv:1511.03339 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 14 pages. Accepted to appear at CVPR 2016},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\GTBVVP9G\\Chen et al. - 2016 - Attention to Scale Scale-aware Semantic Image Seg.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\MJJCRZWU\\1511.html:text/html},
}

@book{zhang_dive_2019,
	title = {Dive into {Deep} {Learning}},
	author = {Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},
	year = {2019},
	annote = {http://www.d2l.ai},
}

@article{radford_language_2019,
	title = {Language {Models} are {Unsupervised} {Multitask} {Learners}},
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts WebText. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	language = {en},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	year = {2019},
	pages = {24},
	file = {Radford et al. - Language Models are Unsupervised Multitask Learner.pdf:C\:\\Users\\morit\\Zotero\\storage\\BV85TTMU\\Radford et al. - Language Models are Unsupervised Multitask Learner.pdf:application/pdf},
}

@techreport{poudel_fast-scnn_2019,
	title = {Fast-{SCNN}: {Fast} {Semantic} {Segmentation} {Network}},
	shorttitle = {Fast-{SCNN}},
	url = {http://arxiv.org/abs/1902.04502},
	abstract = {The encoder-decoder framework is state-of-the-art for offline semantic image segmentation. Since the rise in autonomous systems, real-time computation is increasingly desirable. In this paper, we introduce fast segmentation convolutional neural network (Fast-SCNN), an above real-time semantic segmentation model on high resolution image data (1024x2048px) suited to efficient computation on embedded devices with low memory. Building on existing two-branch methods for fast segmentation, we introduce our `learning to downsample' module which computes low-level features for multiple resolution branches simultaneously. Our network combines spatial detail at high resolution with deep features extracted at lower resolution, yielding an accuracy of 68.0\% mean intersection over union at 123.5 frames per second on Cityscapes. We also show that large scale pre-training is unnecessary. We thoroughly validate our metric in experiments with ImageNet pre-training and the coarse labeled data of Cityscapes. Finally, we show even faster computation with competitive results on subsampled inputs, without any network modifications.},
	number = {arXiv:1902.04502},
	urldate = {2022-06-04},
	institution = {arXiv},
	author = {Poudel, Rudra P. K. and Liwicki, Stephan and Cipolla, Roberto},
	month = feb,
	year = {2019},
	doi = {10.48550/arXiv.1902.04502},
	note = {arXiv:1902.04502 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\VW3ZCDHC\\Poudel et al. - 2019 - Fast-SCNN Fast Semantic Segmentation Network.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\YLFB4DGE\\1902.html:text/html},
}

@inproceedings{chen_learning_2017,
	title = {Learning {Efficient} {Object} {Detection} {Models} with {Knowledge} {Distillation}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/e1e32e235eee1f970470a3a6658dfdd5-Abstract.html},
	abstract = {Despite significant accuracy improvement in convolutional neural networks (CNN) based object detectors, they often require prohibitive runtimes to process an image for real-time applications. State-of-the-art models often use very deep networks with a large number of floating point operations. Efforts such as model compression learn compact models with fewer number of parameters, but with much reduced accuracy. In this work, we propose a new framework to learn compact and fast ob- ject detection networks with improved accuracy using knowledge distillation [20] and hint learning [34]. Although knowledge distillation has demonstrated excellent improvements for simpler classification setups, the complexity of detection poses new challenges in the form of regression, region proposals and less voluminous la- bels. We address this through several innovations such as a weighted cross-entropy loss to address class imbalance, a teacher bounded loss to handle the regression component and adaptation layers to better learn from intermediate teacher distribu- tions. We conduct comprehensive empirical evaluation with different distillation configurations over multiple datasets including PASCAL, KITTI, ILSVRC and MS-COCO. Our results show consistent improvement in accuracy-speed trade-offs for modern multi-class detection models.},
	urldate = {2022-06-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Chen, Guobin and Choi, Wongun and Yu, Xiang and Han, Tony and Chandraker, Manmohan},
	year = {2017},
	file = {Full Text PDF:C\:\\Users\\morit\\Zotero\\storage\\E8UVRRWW\\Chen et al. - 2017 - Learning Efficient Object Detection Models with Kn.pdf:application/pdf},
}

@techreport{bai_dynamically_2021,
	title = {Dynamically pruning segformer for efficient semantic segmentation},
	url = {http://arxiv.org/abs/2111.09499},
	abstract = {As one of the successful Transformer-based models in computer vision tasks, SegFormer demonstrates superior performance in semantic segmentation. Nevertheless, the high computational cost greatly challenges the deployment of SegFormer on edge devices. In this paper, we seek to design a lightweight SegFormer for efficient semantic segmentation. Based on the observation that neurons in SegFormer layers exhibit large variances across different images, we propose a dynamic gated linear layer, which prunes the most uninformative set of neurons based on the input instance. To improve the dynamically pruned SegFormer, we also introduce two-stage knowledge distillation to transfer the knowledge within the original teacher to the pruned student network. Experimental results show that our method can significantly reduce the computation overhead of SegFormer without an apparent performance drop. For instance, we can achieve 36.9\% mIoU with only 3.3G FLOPs on ADE20K, saving more than 60\% computation with the drop of only 0.5\% in mIoU},
	number = {arXiv:2111.09499},
	urldate = {2022-06-04},
	institution = {arXiv},
	author = {Bai, Haoli and Mao, Hongda and Nair, Dinesh},
	month = nov,
	year = {2021},
	doi = {10.48550/arXiv.2111.09499},
	note = {arXiv:2111.09499 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\WQ9E4MNJ\\Bai et al. - 2021 - Dynamically pruning segformer for efficient semant.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\RAZTXT8J\\2111.html:text/html},
}

@techreport{zhou_semantic_2018,
	title = {Semantic {Understanding} of {Scenes} through the {ADE20K} {Dataset}},
	url = {http://arxiv.org/abs/1608.05442},
	abstract = {Scene parsing, or recognizing and segmenting objects and stuff in an image, is one of the key problems in computer vision. Despite the community's efforts in data collection, there are still few image datasets covering a wide range of scenes and object categories with dense and detailed annotations for scene parsing. In this paper, we introduce and analyze the ADE20K dataset, spanning diverse annotations of scenes, objects, parts of objects, and in some cases even parts of parts. A generic network design called Cascade Segmentation Module is then proposed to enable the segmentation networks to parse a scene into stuff, objects, and object parts in a cascade. We evaluate the proposed module integrated within two existing semantic segmentation networks, yielding significant improvements for scene parsing. We further show that the scene parsing networks trained on ADE20K can be applied to a wide variety of scenes and objects.},
	number = {arXiv:1608.05442},
	urldate = {2022-06-04},
	institution = {arXiv},
	author = {Zhou, Bolei and Zhao, Hang and Puig, Xavier and Xiao, Tete and Fidler, Sanja and Barriuso, Adela and Torralba, Antonio},
	month = oct,
	year = {2018},
	doi = {10.48550/arXiv.1608.05442},
	note = {arXiv:1608.05442 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: IJCV extension},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\JE4NMGAB\\Zhou et al. - 2018 - Semantic Understanding of Scenes through the ADE20.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\3XTLY52E\\1608.html:text/html},
}

@techreport{wang_pyramid_2021,
	title = {Pyramid {Vision} {Transformer}: {A} {Versatile} {Backbone} for {Dense} {Prediction} without {Convolutions}},
	shorttitle = {Pyramid {Vision} {Transformer}},
	url = {http://arxiv.org/abs/2102.12122},
	abstract = {Although using convolutional neural networks (CNNs) as backbones achieves great successes in computer vision, this work investigates a simple backbone network useful for many dense prediction tasks without convolutions. Unlike the recently-proposed Transformer model (e.g., ViT) that is specially designed for image classification, we propose Pyramid Vision Transformer{\textasciitilde}(PVT), which overcomes the difficulties of porting Transformer to various dense prediction tasks. PVT has several merits compared to prior arts. (1) Different from ViT that typically has low-resolution outputs and high computational and memory cost, PVT can be not only trained on dense partitions of the image to achieve high output resolution, which is important for dense predictions but also using a progressive shrinking pyramid to reduce computations of large feature maps. (2) PVT inherits the advantages from both CNN and Transformer, making it a unified backbone in various vision tasks without convolutions by simply replacing CNN backbones. (3) We validate PVT by conducting extensive experiments, showing that it boosts the performance of many downstream tasks, e.g., object detection, semantic, and instance segmentation. For example, with a comparable number of parameters, RetinaNet+PVT achieves 40.4 AP on the COCO dataset, surpassing RetinNet+ResNet50 (36.3 AP) by 4.1 absolute AP. We hope PVT could serve as an alternative and useful backbone for pixel-level predictions and facilitate future researches. Code is available at https://github.com/whai362/PVT.},
	number = {arXiv:2102.12122},
	urldate = {2022-06-04},
	institution = {arXiv},
	author = {Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},
	month = aug,
	year = {2021},
	note = {arXiv:2102.12122 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted to ICCV 2021},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\QURXSQJ7\\Wang et al. - 2021 - Pyramid Vision Transformer A Versatile Backbone f.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\S63BX44N\\2102.html:text/html},
}

@techreport{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	number = {arXiv:1810.04805},
	urldate = {2022-06-04},
	institution = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	doi = {10.48550/arXiv.1810.04805},
	note = {arXiv:1810.04805 [cs]
type: article},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\KT5XKU8C\\Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\MCDBUWX6\\1810.html:text/html},
}

@techreport{dosovitskiy_image_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	number = {arXiv:2010.11929},
	urldate = {2022-06-04},
	institution = {arXiv},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = jun,
	year = {2021},
	doi = {10.48550/arXiv.2010.11929},
	note = {arXiv:2010.11929 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: Fine-tuning code and pre-trained models are available at https://github.com/google-research/vision\_transformer. ICLR camera-ready version with 2 small modifications: 1) Added a discussion of CLS vs GAP classifier in the appendix, 2) Fixed an error in exaFLOPs computation in Figure 5 and Table 6 (relative performance of models is basically not affected)},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\XKYFKUXV\\Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Im.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\MNE7YRRB\\2010.html:text/html},
}

@techreport{sandler_mobilenetv2_2019,
	title = {{MobileNetV2}: {Inverted} {Residuals} and {Linear} {Bottlenecks}},
	shorttitle = {{MobileNetV2}},
	url = {http://arxiv.org/abs/1801.04381},
	abstract = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters},
	number = {arXiv:1801.04381},
	urldate = {2022-06-04},
	institution = {arXiv},
	author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
	month = mar,
	year = {2019},
	doi = {10.48550/arXiv.1801.04381},
	note = {arXiv:1801.04381 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\VRWPIDZU\\Sandler et al. - 2019 - MobileNetV2 Inverted Residuals and Linear Bottlen.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\YBHCFHGX\\1801.html:text/html},
}

@techreport{howard_mobilenets_2017,
	title = {{MobileNets}: {Efficient} {Convolutional} {Neural} {Networks} for {Mobile} {Vision} {Applications}},
	shorttitle = {{MobileNets}},
	url = {http://arxiv.org/abs/1704.04861},
	abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
	number = {arXiv:1704.04861},
	urldate = {2022-06-04},
	institution = {arXiv},
	author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
	month = apr,
	year = {2017},
	doi = {10.48550/arXiv.1704.04861},
	note = {arXiv:1704.04861 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\5IP7Z9CL\\Howard et al. - 2017 - MobileNets Efficient Convolutional Neural Network.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\W4PRKUXT\\1704.html:text/html},
}

@techreport{chen_encoder-decoder_2018,
	title = {Encoder-{Decoder} with {Atrous} {Separable} {Convolution} for {Semantic} {Image} {Segmentation}},
	url = {http://arxiv.org/abs/1802.02611},
	abstract = {Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89.0{\textbackslash}\% and 82.1{\textbackslash}\% without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at {\textbackslash}url\{https://github.com/tensorflow/models/tree/master/research/deeplab\}.},
	number = {arXiv:1802.02611},
	urldate = {2022-06-04},
	institution = {arXiv},
	author = {Chen, Liang-Chieh and Zhu, Yukun and Papandreou, George and Schroff, Florian and Adam, Hartwig},
	month = aug,
	year = {2018},
	doi = {10.48550/arXiv.1802.02611},
	note = {arXiv:1802.02611 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: ECCV 2018 camera ready},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\9ZXCY2CU\\Chen et al. - 2018 - Encoder-Decoder with Atrous Separable Convolution .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\8A3VEJDW\\1802.html:text/html},
}

@techreport{chen_rethinking_2017,
	title = {Rethinking {Atrous} {Convolution} for {Semantic} {Image} {Segmentation}},
	url = {http://arxiv.org/abs/1706.05587},
	abstract = {In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.},
	number = {arXiv:1706.05587},
	urldate = {2022-06-04},
	institution = {arXiv},
	author = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
	month = dec,
	year = {2017},
	doi = {10.48550/arXiv.1706.05587},
	note = {arXiv:1706.05587 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Add more experimental results},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\GU7TH53Q\\Chen et al. - 2017 - Rethinking Atrous Convolution for Semantic Image S.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\BAPFFUA9\\1706.html:text/html},
}

@techreport{zhao_icnet_2018,
	title = {{ICNet} for {Real}-{Time} {Semantic} {Segmentation} on {High}-{Resolution} {Images}},
	url = {http://arxiv.org/abs/1704.08545},
	abstract = {We focus on the challenging task of real-time semantic segmentation in this paper. It finds many practical applications and yet is with fundamental difficulty of reducing a large portion of computation for pixel-wise label inference. We propose an image cascade network (ICNet) that incorporates multi-resolution branches under proper label guidance to address this challenge. We provide in-depth analysis of our framework and introduce the cascade feature fusion unit to quickly achieve high-quality segmentation. Our system yields real-time inference on a single GPU card with decent quality results evaluated on challenging datasets like Cityscapes, CamVid and COCO-Stuff.},
	number = {arXiv:1704.08545},
	urldate = {2022-06-04},
	institution = {arXiv},
	author = {Zhao, Hengshuang and Qi, Xiaojuan and Shen, Xiaoyong and Shi, Jianping and Jia, Jiaya},
	month = aug,
	year = {2018},
	doi = {10.48550/arXiv.1704.08545},
	note = {arXiv:1704.08545 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: ECCV 2018},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\LSLXA7KT\\Zhao et al. - 2018 - ICNet for Real-Time Semantic Segmentation on High-.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\V7DVS23I\\1704.html:text/html},
}

@techreport{chen_semantic_2016,
	title = {Semantic {Image} {Segmentation} with {Deep} {Convolutional} {Nets} and {Fully} {Connected} {CRFs}},
	url = {http://arxiv.org/abs/1412.7062},
	abstract = {Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called "semantic image segmentation"). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our "DeepLab" system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6\% IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the 'hole' algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU.},
	number = {arXiv:1412.7062},
	urldate = {2022-06-04},
	institution = {arXiv},
	author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
	month = jun,
	year = {2016},
	doi = {10.48550/arXiv.1412.7062},
	note = {arXiv:1412.7062 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: 14 pages. Updated related work},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\PN47HVKW\\Chen et al. - 2016 - Semantic Image Segmentation with Deep Convolutiona.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\J9Z6NMGF\\1412.html:text/html},
}

@techreport{liu_parsenet_2015,
	title = {{ParseNet}: {Looking} {Wider} to {See} {Better}},
	shorttitle = {{ParseNet}},
	url = {http://arxiv.org/abs/1506.04579},
	abstract = {We present a technique for adding global context to deep convolutional networks for semantic segmentation. The approach is simple, using the average feature for a layer to augment the features at each location. In addition, we study several idiosyncrasies of training, significantly increasing the performance of baseline networks (e.g. from FCN). When we add our proposed global feature, and a technique for learning normalization parameters, accuracy increases consistently even over our improved versions of the baselines. Our proposed approach, ParseNet, achieves state-of-the-art performance on SiftFlow and PASCAL-Context with small additional computational cost over baselines, and near current state-of-the-art performance on PASCAL VOC 2012 semantic segmentation with a simple approach. Code is available at https://github.com/weiliu89/caffe/tree/fcn .},
	number = {arXiv:1506.04579},
	urldate = {2022-06-04},
	institution = {arXiv},
	author = {Liu, Wei and Rabinovich, Andrew and Berg, Alexander C.},
	month = nov,
	year = {2015},
	doi = {10.48550/arXiv.1506.04579},
	note = {arXiv:1506.04579 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: ICLR 2016 submission},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\W2SNMFTR\\Liu et al. - 2015 - ParseNet Looking Wider to See Better.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\I4ID4SQV\\1506.html:text/html},
}

@techreport{sun_revisiting_2017,
	title = {Revisiting {Unreasonable} {Effectiveness} of {Data} in {Deep} {Learning} {Era}},
	url = {http://arxiv.org/abs/1707.02968},
	abstract = {The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10x or 100x? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between `enormous data' and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.},
	number = {arXiv:1707.02968},
	urldate = {2022-06-04},
	institution = {arXiv},
	author = {Sun, Chen and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
	month = aug,
	year = {2017},
	doi = {10.48550/arXiv.1707.02968},
	note = {arXiv:1707.02968 [cs]
version: 2
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	annote = {Comment: ICCV 2017 camera ready},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\EUBGRLLF\\Sun et al. - 2017 - Revisiting Unreasonable Effectiveness of Data in D.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\B8J883YW\\1707.html:text/html},
}

@misc{noauthor_papers_nodate-1,
	title = {Papers with {Code} - {Revisiting} {Unreasonable} {Effectiveness} of {Data} in {Deep} {Learning} {Era}},
	url = {https://paperswithcode.com/paper/revisiting-unreasonable-effectiveness-of-data},
	abstract = {\#2 best model for Semantic Segmentation on PASCAL VOC 2007 (Mean IoU metric)},
	language = {en},
	urldate = {2022-06-04},
	file = {Snapshot:C\:\\Users\\morit\\Zotero\\storage\\PZVJGM3F\\revisiting-unreasonable-effectiveness-of-data.html:text/html},
}

@techreport{russakovsky_imagenet_2015,
	title = {{ImageNet} {Large} {Scale} {Visual} {Recognition} {Challenge}},
	url = {http://arxiv.org/abs/1409.0575},
	abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
	number = {arXiv:1409.0575},
	urldate = {2022-06-04},
	institution = {arXiv},
	author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
	month = jan,
	year = {2015},
	doi = {10.48550/arXiv.1409.0575},
	note = {arXiv:1409.0575 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, I.4.8, I.5.2},
	annote = {Comment: 43 pages, 16 figures. v3 includes additional comparisons with PASCAL VOC (per-category comparisons in Table 3, distribution of localization difficulty in Fig 16), a list of queries used for obtaining object detection images (Appendix C), and some additional references},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\TNKI33SR\\Russakovsky et al. - 2015 - ImageNet Large Scale Visual Recognition Challenge.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\I37DIWM5\\1409.html:text/html},
}

@techreport{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	number = {arXiv:1512.03385},
	urldate = {2022-06-04},
	institution = {arXiv},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	doi = {10.48550/arXiv.1512.03385},
	note = {arXiv:1512.03385 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Tech report},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\5TLUE6R8\\He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\JQ2TV2X9\\1512.html:text/html},
}

@techreport{simonyan_very_2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	number = {arXiv:1409.1556},
	urldate = {2022-06-04},
	institution = {arXiv},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = apr,
	year = {2015},
	doi = {10.48550/arXiv.1409.1556},
	note = {arXiv:1409.1556 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\FGRNXRPD\\Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\BX2IFLT9\\1409.html:text/html},
}

@techreport{girshick_rich_2014,
	title = {Rich feature hierarchies for accurate object detection and semantic segmentation},
	url = {http://arxiv.org/abs/1311.2524},
	abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012---achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/{\textasciitilde}rbg/rcnn.},
	number = {arXiv:1311.2524},
	urldate = {2022-06-04},
	institution = {arXiv},
	author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
	month = oct,
	year = {2014},
	doi = {10.48550/arXiv.1311.2524},
	note = {arXiv:1311.2524 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Extended version of our CVPR 2014 paper; latest update (v5) includes results using deeper networks (see Appendix G. Changelog)},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\HD5HXFDX\\Girshick et al. - 2014 - Rich feature hierarchies for accurate object detec.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\H5SV6P9P\\1311.html:text/html},
}

@techreport{richter_enhancing_2021,
	title = {Enhancing {Photorealism} {Enhancement}},
	url = {http://arxiv.org/abs/2105.04619},
	abstract = {We present an approach to enhancing the realism of synthetic images. The images are enhanced by a convolutional network that leverages intermediate representations produced by conventional rendering pipelines. The network is trained via a novel adversarial objective, which provides strong supervision at multiple perceptual levels. We analyze scene layout distributions in commonly used datasets and find that they differ in important ways. We hypothesize that this is one of the causes of strong artifacts that can be observed in the results of many prior methods. To address this we propose a new strategy for sampling image patches during training. We also introduce multiple architectural improvements in the deep network modules used for photorealism enhancement. We confirm the benefits of our contributions in controlled experiments and report substantial gains in stability and realism in comparison to recent image-to-image translation methods and a variety of other baselines.},
	number = {arXiv:2105.04619},
	urldate = {2022-06-04},
	institution = {arXiv},
	author = {Richter, Stephan R. and AlHaija, Hassan Abu and Koltun, Vladlen},
	month = may,
	year = {2021},
	doi = {10.48550/arXiv.2105.04619},
	note = {arXiv:2105.04619 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, I.4.8, Computer Science - Graphics},
	annote = {Comment: Code and data available at https://github.com/intel-isl/PhotorealismEnhancement Video available at https://youtu.be/P1IcaBn3ej0},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\PU5E6NMV\\Richter et al. - 2021 - Enhancing Photorealism Enhancement.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\BF4C74VT\\2105.html:text/html},
}

@techreport{zhao_pyramid_2017,
	title = {Pyramid {Scene} {Parsing} {Network}},
	url = {http://arxiv.org/abs/1612.01105},
	abstract = {Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4\% on PASCAL VOC 2012 and accuracy 80.2\% on Cityscapes.},
	number = {arXiv:1612.01105},
	urldate = {2022-06-04},
	institution = {arXiv},
	author = {Zhao, Hengshuang and Shi, Jianping and Qi, Xiaojuan and Wang, Xiaogang and Jia, Jiaya},
	month = apr,
	year = {2017},
	doi = {10.48550/arXiv.1612.01105},
	note = {arXiv:1612.01105 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR 2017},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\GR9ANDTJ\\Zhao et al. - 2017 - Pyramid Scene Parsing Network.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\HYD27FTF\\1612.html:text/html},
}

@incollection{he_spatial_2014,
	title = {Spatial {Pyramid} {Pooling} in {Deep} {Convolutional} {Networks} for {Visual} {Recognition}},
	volume = {8691},
	url = {http://arxiv.org/abs/1406.4729},
	abstract = {Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224x224) input image. This requirement is "artificial" and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, "spatial pyramid pooling", to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-the-art classification results using a single full-image representation and no fine-tuning. The power of SPP-net is also significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method is 24-102x faster than the R-CNN method, while achieving better or comparable accuracy on Pascal VOC 2007. In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our methods rank \#2 in object detection and \#3 in image classification among all 38 teams. This manuscript also introduces the improvement made for this competition.},
	urldate = {2022-06-04},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2014},
	doi = {10.1007/978-3-319-10578-9_23},
	note = {arXiv:1406.4729 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {346--361},
	annote = {Comment: This manuscript is the accepted version for IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 2015. See Changelog},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\3ARI8DNQ\\He et al. - 2014 - Spatial Pyramid Pooling in Deep Convolutional Netw.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\6QMDG5AV\\1406.html:text/html},
}

@techreport{gupta_learning_2014,
	title = {Learning {Rich} {Features} from {RGB}-{D} {Images} for {Object} {Detection} and {Segmentation}},
	url = {http://arxiv.org/abs/1407.5736},
	abstract = {In this paper we study the problem of object detection for RGB-D images using semantically rich image and depth features. We propose a new geocentric embedding for depth images that encodes height above ground and angle with gravity for each pixel in addition to the horizontal disparity. We demonstrate that this geocentric embedding works better than using raw depth images for learning feature representations with convolutional neural networks. Our final object detection system achieves an average precision of 37.3\%, which is a 56\% relative improvement over existing methods. We then focus on the task of instance segmentation where we label pixels belonging to object instances found by our detector. For this task, we propose a decision forest approach that classifies pixels in the detection window as foreground or background using a family of unary and binary tests that query shape and geocentric pose features. Finally, we use the output from our object detectors in an existing superpixel classification framework for semantic scene segmentation and achieve a 24\% relative improvement over current state-of-the-art for the object categories that we study. We believe advances such as those represented in this paper will facilitate the use of perception in fields like robotics.},
	number = {arXiv:1407.5736},
	urldate = {2022-06-04},
	institution = {arXiv},
	author = {Gupta, Saurabh and Girshick, Ross and Arbeláez, Pablo and Malik, Jitendra},
	month = jul,
	year = {2014},
	note = {arXiv:1407.5736 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	annote = {Comment: To appear in the European Conference on Computer Vision (ECCV), 2014},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\FA25S2SB\\Gupta et al. - 2014 - Learning Rich Features from RGB-D Images for Objec.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\6VR5AHCR\\1407.html:text/html},
}

@misc{noauthor_13112524_nodate,
	title = {[1311.2524] {Rich} feature hierarchies for accurate object detection and semantic segmentation},
	url = {https://arxiv.org/abs/1311.2524},
	urldate = {2022-06-04},
	file = {[1311.2524] Rich feature hierarchies for accurate object detection and semantic segmentation:C\:\\Users\\morit\\Zotero\\storage\\CLU42FNY\\1311.html:text/html},
}

@article{hesamian_deep_2019,
	title = {Deep {Learning} {Techniques} for {Medical} {Image} {Segmentation}: {Achievements} and {Challenges}},
	volume = {32},
	issn = {1618-727X},
	shorttitle = {Deep {Learning} {Techniques} for {Medical} {Image} {Segmentation}},
	url = {https://doi.org/10.1007/s10278-019-00227-x},
	doi = {10.1007/s10278-019-00227-x},
	abstract = {Deep learning-based image segmentation is by now firmly established as a robust tool in image segmentation. It has been widely used to separate homogeneous areas as the first and critical component of diagnosis and treatment pipeline. In this article, we present a critical appraisal of popular methods that have employed deep-learning techniques for medical image segmentation. Moreover, we summarize the most common challenges incurred and suggest possible solutions.},
	language = {en},
	number = {4},
	urldate = {2022-06-04},
	journal = {Journal of Digital Imaging},
	author = {Hesamian, Mohammad Hesam and Jia, Wenjing and He, Xiangjian and Kennedy, Paul},
	month = aug,
	year = {2019},
	keywords = {CNN, Deep learning, Medical image segmentation, Organ segmentation},
	pages = {582--596},
	file = {Full Text PDF:C\:\\Users\\morit\\Zotero\\storage\\MCRTAHLB\\Hesamian et al. - 2019 - Deep Learning Techniques for Medical Image Segment.pdf:application/pdf},
}

@techreport{chu_twins_2021,
	title = {Twins: {Revisiting} the {Design} of {Spatial} {Attention} in {Vision} {Transformers}},
	shorttitle = {Twins},
	url = {http://arxiv.org/abs/2104.13840},
	abstract = {Very recently, a variety of vision transformer architectures for dense prediction tasks have been proposed and they show that the design of spatial attention is critical to their success in these tasks. In this work, we revisit the design of the spatial attention and demonstrate that a carefully-devised yet simple spatial attention mechanism performs favourably against the state-of-the-art schemes. As a result, we propose two vision transformer architectures, namely, Twins-PCPVT and Twins-SVT. Our proposed architectures are highly-efficient and easy to implement, only involving matrix multiplications that are highly optimized in modern deep learning frameworks. More importantly, the proposed architectures achieve excellent performance on a wide range of visual tasks, including image level classification as well as dense detection and segmentation. The simplicity and strong performance suggest that our proposed architectures may serve as stronger backbones for many vision tasks. Our code is released at https://github.com/Meituan-AutoML/Twins .},
	number = {arXiv:2104.13840},
	urldate = {2022-06-04},
	institution = {arXiv},
	author = {Chu, Xiangxiang and Tian, Zhi and Wang, Yuqing and Zhang, Bo and Ren, Haibing and Wei, Xiaolin and Xia, Huaxia and Shen, Chunhua},
	month = sep,
	year = {2021},
	doi = {10.48550/arXiv.2104.13840},
	note = {arXiv:2104.13840 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: Accepted to NeurIPS2021},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\DYTK6LW4\\Chu et al. - 2021 - Twins Revisiting the Design of Spatial Attention .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\8TUYCSNC\\2104.html:text/html},
}

@techreport{liu_swin_2021,
	title = {Swin {Transformer}: {Hierarchical} {Vision} {Transformer} using {Shifted} {Windows}},
	shorttitle = {Swin {Transformer}},
	url = {http://arxiv.org/abs/2103.14030},
	abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with {\textbackslash}textbf\{S\}hifted {\textbackslash}textbf\{win\}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at{\textasciitilde}{\textbackslash}url\{https://github.com/microsoft/Swin-Transformer\}.},
	number = {arXiv:2103.14030},
	urldate = {2022-06-04},
	institution = {arXiv},
	author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
	month = aug,
	year = {2021},
	doi = {10.48550/arXiv.2103.14030},
	note = {arXiv:2103.14030 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\K4RHTN8X\\Liu et al. - 2021 - Swin Transformer Hierarchical Vision Transformer .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\R2JX2YMQ\\2103.html:text/html},
}

@techreport{touvron_training_2021,
	title = {Training data-efficient image transformers \& distillation through attention},
	url = {http://arxiv.org/abs/2012.12877},
	abstract = {Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption. In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1\% (single-crop evaluation) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2\% accuracy) and when transferring to other tasks. We share our code and models.},
	number = {arXiv:2012.12877},
	urldate = {2022-06-04},
	institution = {arXiv},
	author = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jégou, Hervé},
	month = jan,
	year = {2021},
	doi = {10.48550/arXiv.2012.12877},
	note = {arXiv:2012.12877 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\U8VBESST\\Touvron et al. - 2021 - Training data-efficient image transformers & disti.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\NWSFV6AD\\2012.html:text/html},
}

@techreport{poudel_contextnet_2018,
	title = {{ContextNet}: {Exploring} {Context} and {Detail} for {Semantic} {Segmentation} in {Real}-time},
	shorttitle = {{ContextNet}},
	url = {http://arxiv.org/abs/1805.04554},
	abstract = {Modern deep learning architectures produce highly accurate results on many challenging semantic segmentation datasets. State-of-the-art methods are, however, not directly transferable to real-time applications or embedded devices, since naive adaptation of such systems to reduce computational cost (speed, memory and energy) causes a significant drop in accuracy. We propose ContextNet, a new deep neural network architecture which builds on factorized convolution, network compression and pyramid representation to produce competitive semantic segmentation in real-time with low memory requirement. ContextNet combines a deep network branch at low resolution that captures global context information efficiently with a shallow branch that focuses on high-resolution segmentation details. We analyse our network in a thorough ablation study and present results on the Cityscapes dataset, achieving 66.1\% accuracy at 18.3 frames per second at full (1024x2048) resolution (41.9 fps with pipelined computations for streamed data).},
	number = {arXiv:1805.04554},
	urldate = {2022-05-30},
	institution = {arXiv},
	author = {Poudel, Rudra P. K. and Bonde, Ujwal and Liwicki, Stephan and Zach, Christopher},
	month = nov,
	year = {2018},
	doi = {10.48550/arXiv.1805.04554},
	note = {arXiv:1805.04554 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Published as a conference paper at British Machine Vision Conference (BMVC), 2018},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\3KJLAQ96\\Poudel et al. - 2018 - ContextNet Exploring Context and Detail for Seman.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\3J7KELA7\\1805.html:text/html},
}

@techreport{badrinarayanan_segnet_2016,
	title = {{SegNet}: {A} {Deep} {Convolutional} {Encoder}-{Decoder} {Architecture} for {Image} {Segmentation}},
	shorttitle = {{SegNet}},
	url = {http://arxiv.org/abs/1511.00561},
	abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN and also with the well known DeepLab-LargeFOV, DeconvNet architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. We show that SegNet provides good performance with competitive inference time and more efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.},
	number = {arXiv:1511.00561},
	urldate = {2022-05-30},
	institution = {arXiv},
	author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
	month = oct,
	year = {2016},
	doi = {10.48550/arXiv.1511.00561},
	note = {arXiv:1511.00561 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\EQBBT2YV\\Badrinarayanan et al. - 2016 - SegNet A Deep Convolutional Encoder-Decoder Archi.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\NRGARFNN\\1511.html:text/html},
}

@techreport{paszke_enet_2016,
	title = {{ENet}: {A} {Deep} {Neural} {Network} {Architecture} for {Real}-{Time} {Semantic} {Segmentation}},
	shorttitle = {{ENet}},
	url = {http://arxiv.org/abs/1606.02147},
	abstract = {The ability to perform pixel-wise semantic segmentation in real-time is of paramount importance in mobile applications. Recent deep neural networks aimed at this task have the disadvantage of requiring a large number of floating point operations and have long run-times that hinder their usability. In this paper, we propose a novel deep neural network architecture named ENet (efficient neural network), created specifically for tasks requiring low latency operation. ENet is up to 18\${\textbackslash}times\$ faster, requires 75\${\textbackslash}times\$ less FLOPs, has 79\${\textbackslash}times\$ less parameters, and provides similar or better accuracy to existing models. We have tested it on CamVid, Cityscapes and SUN datasets and report on comparisons with existing state-of-the-art methods, and the trade-offs between accuracy and processing time of a network. We present performance measurements of the proposed architecture on embedded systems and suggest possible software improvements that could make ENet even faster.},
	number = {arXiv:1606.02147},
	urldate = {2022-05-30},
	institution = {arXiv},
	author = {Paszke, Adam and Chaurasia, Abhishek and Kim, Sangpil and Culurciello, Eugenio},
	month = jun,
	year = {2016},
	doi = {10.48550/arXiv.1606.02147},
	note = {arXiv:1606.02147 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\DRBPXS9K\\Paszke et al. - 2016 - ENet A Deep Neural Network Architecture for Real-.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\ZE22SVIL\\1606.html:text/html},
}

@techreport{chollet_xception_2017,
	title = {Xception: {Deep} {Learning} with {Depthwise} {Separable} {Convolutions}},
	shorttitle = {Xception},
	url = {http://arxiv.org/abs/1610.02357},
	abstract = {We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.},
	number = {arXiv:1610.02357},
	urldate = {2022-05-30},
	institution = {arXiv},
	author = {Chollet, François},
	month = apr,
	year = {2017},
	note = {arXiv:1610.02357 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\PJ8ZLFPC\\Chollet - 2017 - Xception Deep Learning with Depthwise Separable C.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\RH3GCR5K\\1610.html:text/html},
}

@techreport{yang_tvt_2021,
	title = {{TVT}: {Transferable} {Vision} {Transformer} for {Unsupervised} {Domain} {Adaptation}},
	shorttitle = {{TVT}},
	url = {http://arxiv.org/abs/2108.05988},
	abstract = {Unsupervised domain adaptation (UDA) aims to transfer the knowledge learnt from a labeled source domain to an unlabeled target domain. Previous work is mainly built upon convolutional neural networks (CNNs) to learn domain-invariant representations. With the recent exponential increase in applying Vision Transformer (ViT) to vision tasks, the capability of ViT in adapting cross-domain knowledge, however, remains unexplored in the literature. To fill this gap, this paper first comprehensively investigates the transferability of ViT on a variety of domain adaptation tasks. Surprisingly, ViT demonstrates superior transferability over its CNNs-based counterparts with a large margin, while the performance can be further improved by incorporating adversarial adaptation. Notwithstanding, directly using CNNs-based adaptation strategies fails to take the advantage of ViT's intrinsic merits (e.g., attention mechanism and sequential image representation) which play an important role in knowledge transfer. To remedy this, we propose an unified framework, namely Transferable Vision Transformer (TVT), to fully exploit the transferability of ViT for domain adaptation. Specifically, we delicately devise a novel and effective unit, which we term Transferability Adaption Module (TAM). By injecting learned transferabilities into attention blocks, TAM compels ViT focus on both transferable and discriminative features. Besides, we leverage discriminative clustering to enhance feature diversity and separation which are undermined during adversarial domain alignment. To verify its versatility, we perform extensive studies of TVT on four benchmarks and the experimental results demonstrate that TVT attains significant improvements compared to existing state-of-the-art UDA methods.},
	number = {arXiv:2108.05988},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Yang, Jinyu and Liu, Jingjing and Xu, Ning and Huang, Junzhou},
	month = nov,
	year = {2021},
	doi = {10.48550/arXiv.2108.05988},
	note = {arXiv:2108.05988 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Code: https://github.com/uta-smile/TVT},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\6BZT5FF4\\Yang et al. - 2021 - TVT Transferable Vision Transformer for Unsupervi.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\J2FIPBMV\\2108.html:text/html},
}

@techreport{zou_domain_2018,
	title = {Domain {Adaptation} for {Semantic} {Segmentation} via {Class}-{Balanced} {Self}-{Training}},
	url = {http://arxiv.org/abs/1810.07911},
	abstract = {Recent deep networks achieved state of the art performance on a variety of semantic segmentation tasks. Despite such progress, these models often face challenges in real world `wild tasks' where large difference between labeled training/source data and unseen test/target data exists. In particular, such difference is often referred to as `domain gap', and could cause significantly decreased performance which cannot be easily remedied by further increasing the representation power. Unsupervised domain adaptation (UDA) seeks to overcome such problem without target domain labels. In this paper, we propose a novel UDA framework based on an iterative self-training procedure, where the problem is formulated as latent variable loss minimization, and can be solved by alternatively generating pseudo labels on target data and re-training the model with these labels. On top of self-training, we also propose a novel class-balanced self-training framework to avoid the gradual dominance of large classes on pseudo-label generation, and introduce spatial priors to refine generated labels. Comprehensive experiments show that the proposed methods achieve state of the art semantic segmentation performance under multiple major UDA settings.},
	number = {arXiv:1810.07911},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Zou, Yang and Yu, Zhiding and Kumar, B. V. K. Vijaya and Wang, Jinsong},
	month = oct,
	year = {2018},
	doi = {10.48550/arXiv.1810.07911},
	note = {arXiv:1810.07911 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Multimedia},
	annote = {Comment: Accepted to ECCV 2018},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\K3LXSWS8\\Zou et al. - 2018 - Domain Adaptation for Semantic Segmentation via Cl.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\QWV5FXDN\\1810.html:text/html},
}

@techreport{ganin_domain-adversarial_2016,
	title = {Domain-{Adversarial} {Training} of {Neural} {Networks}},
	url = {http://arxiv.org/abs/1505.07818},
	abstract = {We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains. The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages. We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application.},
	number = {arXiv:1505.07818},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, François and Marchand, Mario and Lempitsky, Victor},
	month = may,
	year = {2016},
	doi = {10.48550/arXiv.1505.07818},
	note = {arXiv:1505.07818 [cs, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: Published in JMLR: http://jmlr.org/papers/v17/15-239.html},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\X2Z9BPCL\\Ganin et al. - 2016 - Domain-Adversarial Training of Neural Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\MICFVMHL\\1505.html:text/html},
}

@article{rozantsev_beyond_2019,
	title = {Beyond {Sharing} {Weights} for {Deep} {Domain} {Adaptation}},
	volume = {41},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {http://arxiv.org/abs/1603.06432},
	doi = {10.1109/TPAMI.2018.2814042},
	abstract = {The performance of a classifier trained on data coming from a specific domain typically degrades when applied to a related but different one. While annotating many samples from the new domain would address this issue, it is often too expensive or impractical. Domain Adaptation has therefore emerged as a solution to this problem; It leverages annotated data from a source domain, in which it is abundant, to train a classifier to operate in a target domain, in which it is either sparse or even lacking altogether. In this context, the recent trend consists of learning deep architectures whose weights are shared for both domains, which essentially amounts to learning domain invariant features. Here, we show that it is more effective to explicitly model the shift from one domain to the other. To this end, we introduce a two-stream architecture, where one operates in the source domain and the other in the target domain. In contrast to other approaches, the weights in corresponding layers are related but not shared. We demonstrate that this both yields higher accuracy than state-of-the-art methods on several object recognition and detection tasks and consistently outperforms networks with shared weights in both supervised and unsupervised settings.},
	number = {4},
	urldate = {2022-06-09},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Rozantsev, Artem and Salzmann, Mathieu and Fua, Pascal},
	month = apr,
	year = {2019},
	note = {arXiv:1603.06432 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {801--814},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\JVHJZBYD\\Rozantsev et al. - 2019 - Beyond Sharing Weights for Deep Domain Adaptation.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\IKVNTX79\\1603.html:text/html},
}

@techreport{mirza_conditional_2014,
	title = {Conditional {Generative} {Adversarial} {Nets}},
	url = {http://arxiv.org/abs/1411.1784},
	abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
	number = {arXiv:1411.1784},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Mirza, Mehdi and Osindero, Simon},
	month = nov,
	year = {2014},
	doi = {10.48550/arXiv.1411.1784},
	note = {arXiv:1411.1784 [cs, stat]
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\LJB8D2WB\\Mirza and Osindero - 2014 - Conditional Generative Adversarial Nets.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\2BQE5DGM\\1411.html:text/html},
}

@techreport{zhu_unpaired_2020,
	title = {Unpaired {Image}-to-{Image} {Translation} using {Cycle}-{Consistent} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1703.10593},
	abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain \$X\$ to a target domain \$Y\$ in the absence of paired examples. Our goal is to learn a mapping \$G: X {\textbackslash}rightarrow Y\$ such that the distribution of images from \$G(X)\$ is indistinguishable from the distribution \$Y\$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping \$F: Y {\textbackslash}rightarrow X\$ and introduce a cycle consistency loss to push \$F(G(X)) {\textbackslash}approx X\$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
	number = {arXiv:1703.10593},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
	month = aug,
	year = {2020},
	doi = {10.48550/arXiv.1703.10593},
	note = {arXiv:1703.10593 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: An extended version of our ICCV 2017 paper, v7 fixed the typos and updated the implementation details. Code and data: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\SG5UAWHU\\Zhu et al. - 2020 - Unpaired Image-to-Image Translation using Cycle-Co.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\WJV47IRZ\\1703.html:text/html},
}

@techreport{kamnitsas_transductive_2021,
	title = {Transductive image segmentation: {Self}-training and effect of uncertainty estimation},
	shorttitle = {Transductive image segmentation},
	url = {http://arxiv.org/abs/2107.08964},
	abstract = {Semi-supervised learning (SSL) uses unlabeled data during training to learn better models. Previous studies on SSL for medical image segmentation focused mostly on improving model generalization to unseen data. In some applications, however, our primary interest is not generalization but to obtain optimal predictions on a specific unlabeled database that is fully available during model development. Examples include population studies for extracting imaging phenotypes. This work investigates an often overlooked aspect of SSL, transduction. It focuses on the quality of predictions made on the unlabeled data of interest when they are included for optimization during training, rather than improving generalization. We focus on the self-training framework and explore its potential for transduction. We analyze it through the lens of Information Gain and reveal that learning benefits from the use of calibrated or under-confident models. Our extensive experiments on a large MRI database for multi-class segmentation of traumatic brain lesions shows promising results when comparing transductive with inductive predictions. We believe this study will inspire further research on transductive learning, a well-suited paradigm for medical image analysis.},
	number = {arXiv:2107.08964},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Kamnitsas, Konstantinos and Winzeck, Stefan and Kornaropoulos, Evgenios N. and Whitehouse, Daniel and Englman, Cameron and Phyu, Poe and Pao, Norman and Menon, David K. and Rueckert, Daniel and Das, Tilak and Newcombe, Virginia F. J. and Glocker, Ben},
	month = aug,
	year = {2021},
	doi = {10.48550/arXiv.2107.08964},
	note = {arXiv:2107.08964 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Published at Domain Adaptation and Representation Transfer (DART) wshop at MICCAI 2021. This version improves methods' names and adds 1 experiment in Tab.3a},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\BPBJCR38\\Kamnitsas et al. - 2021 - Transductive image segmentation Self-training and.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\DJ5ZCF68\\2107.html:text/html},
}

@techreport{liang_we_2021,
	title = {Do {We} {Really} {Need} to {Access} the {Source} {Data}? {Source} {Hypothesis} {Transfer} for {Unsupervised} {Domain} {Adaptation}},
	shorttitle = {Do {We} {Really} {Need} to {Access} the {Source} {Data}?},
	url = {http://arxiv.org/abs/2002.08546},
	abstract = {Unsupervised domain adaptation (UDA) aims to leverage the knowledge learned from a labeled source dataset to solve similar tasks in a new unlabeled domain. Prior UDA methods typically require to access the source data when learning to adapt the model, making them risky and inefficient for decentralized private data. This work tackles a practical setting where only a trained source model is available and investigates how we can effectively utilize such a model without source data to solve UDA problems. We propose a simple yet generic representation learning framework, named {\textbackslash}emph\{Source HypOthesis Transfer\} (SHOT). SHOT freezes the classifier module (hypothesis) of the source model and learns the target-specific feature extraction module by exploiting both information maximization and self-supervised pseudo-labeling to implicitly align representations from the target domains to the source hypothesis. To verify its versatility, we evaluate SHOT in a variety of adaptation cases including closed-set, partial-set, and open-set domain adaptation. Experiments indicate that SHOT yields state-of-the-art results among multiple domain adaptation benchmarks.},
	number = {arXiv:2002.08546},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Liang, Jian and Hu, Dapeng and Feng, Jiashi},
	month = jun,
	year = {2021},
	doi = {10.48550/arXiv.2002.08546},
	note = {arXiv:2002.08546 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: ICML2020. Fix the typos for Digits. Code is available at https://github.com/tim-learn/SHOT},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\XADRC3ZY\\Liang et al. - 2021 - Do We Really Need to Access the Source Data Sourc.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\C4KZ4VR2\\2002.html:text/html},
}

@techreport{csurka_unsupervised_2021,
	title = {Unsupervised {Domain} {Adaptation} for {Semantic} {Image} {Segmentation}: a {Comprehensive} {Survey}},
	shorttitle = {Unsupervised {Domain} {Adaptation} for {Semantic} {Image} {Segmentation}},
	url = {http://arxiv.org/abs/2112.03241},
	abstract = {Semantic segmentation plays a fundamental role in a broad variety of computer vision applications, providing key information for the global understanding of an image. Yet, the state-of-the-art models rely on large amount of annotated samples, which are more expensive to obtain than in tasks such as image classification. Since unlabelled data is instead significantly cheaper to obtain, it is not surprising that Unsupervised Domain Adaptation reached a broad success within the semantic segmentation community. This survey is an effort to summarize five years of this incredibly rapidly growing field, which embraces the importance of semantic segmentation itself and a critical need of adapting segmentation models to new environments. We present the most important semantic segmentation methods; we provide a comprehensive survey on domain adaptation techniques for semantic segmentation; we unveil newer trends such as multi-domain learning, domain generalization, test-time adaptation or source-free domain adaptation; we conclude this survey by describing datasets and benchmarks most widely used in semantic segmentation research. We hope that this survey will provide researchers across academia and industry with a comprehensive reference guide and will help them in fostering new research directions in the field.},
	number = {arXiv:2112.03241},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Csurka, Gabriela and Volpi, Riccardo and Chidlovskii, Boris},
	month = dec,
	year = {2021},
	doi = {10.48550/arXiv.2112.03241},
	note = {arXiv:2112.03241 [cs]
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, I.2, I.4.6},
	annote = {Comment: 33 pages},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\YTZGAN7K\\Csurka et al. - 2021 - Unsupervised Domain Adaptation for Semantic Image .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\MCDAYZ8M\\2112.html:text/html},
}

@techreport{hoffman_fcns_2016,
	title = {{FCNs} in the {Wild}: {Pixel}-level {Adversarial} and {Constraint}-based {Adaptation}},
	shorttitle = {{FCNs} in the {Wild}},
	url = {http://arxiv.org/abs/1612.02649},
	abstract = {Fully convolutional models for dense prediction have proven successful for a wide range of visual tasks. Such models perform well in a supervised setting, but performance can be surprisingly poor under domain shifts that appear mild to a human observer. For example, training on one city and testing on another in a different geographic region and/or weather condition may result in significantly degraded performance due to pixel-level distribution shift. In this paper, we introduce the first domain adaptive semantic segmentation method, proposing an unsupervised adversarial approach to pixel prediction problems. Our method consists of both global and category specific adaptation techniques. Global domain alignment is performed using a novel semantic segmentation network with fully convolutional domain adversarial learning. This initially adapted space then enables category specific adaptation through a generalization of constrained weak learning, with explicit transfer of the spatial layout from the source to the target domains. Our approach outperforms baselines across different settings on multiple large-scale datasets, including adapting across various real city environments, different synthetic sub-domains, from simulated to real environments, and on a novel large-scale dash-cam dataset.},
	number = {arXiv:1612.02649},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Hoffman, Judy and Wang, Dequan and Yu, Fisher and Darrell, Trevor},
	month = dec,
	year = {2016},
	doi = {10.48550/arXiv.1612.02649},
	note = {arXiv:1612.02649 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\MTKYPKKZ\\Hoffman et al. - 2016 - FCNs in the Wild Pixel-level Adversarial and Cons.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\G6LVH7B2\\1612.html:text/html},
}

@misc{noauthor_ieee_nodate,
	title = {{IEEE} {Xplore} {Full}-{Text} {PDF}:},
	url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8363602&tag=1},
	urldate = {2022-06-09},
	file = {IEEE Xplore Full-Text PDF\::C\:\\Users\\morit\\Zotero\\storage\\IJJIHRH6\\stamp.html:text/html},
}

@techreport{wu_dannet_2021,
	title = {{DANNet}: {A} {One}-{Stage} {Domain} {Adaptation} {Network} for {Unsupervised} {Nighttime} {Semantic} {Segmentation}},
	shorttitle = {{DANNet}},
	url = {http://arxiv.org/abs/2104.10834},
	abstract = {Semantic segmentation of nighttime images plays an equally important role as that of daytime images in autonomous driving, but the former is much more challenging due to poor illuminations and arduous human annotations. In this paper, we propose a novel domain adaptation network (DANNet) for nighttime semantic segmentation without using labeled nighttime image data. It employs an adversarial training with a labeled daytime dataset and an unlabeled dataset that contains coarsely aligned day-night image pairs. Specifically, for the unlabeled day-night image pairs, we use the pixel-level predictions of static object categories on a daytime image as a pseudo supervision to segment its counterpart nighttime image. We further design a re-weighting strategy to handle the inaccuracy caused by misalignment between day-night image pairs and wrong predictions of daytime images, as well as boost the prediction accuracy of small objects. The proposed DANNet is the first one stage adaptation framework for nighttime semantic segmentation, which does not train additional day-night image transfer models as a separate pre-processing stage. Extensive experiments on Dark Zurich and Nighttime Driving datasets show that our method achieves state-of-the-art performance for nighttime semantic segmentation.},
	number = {arXiv:2104.10834},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Wu, Xinyi and Wu, Zhenyao and Guo, Hao and Ju, Lili and Wang, Song},
	month = apr,
	year = {2021},
	doi = {10.48550/arXiv.2104.10834},
	note = {arXiv:2104.10834 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted by CVPR 2021},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\I3UNZ9D3\\Wu et al. - 2021 - DANNet A One-Stage Domain Adaptation Network for .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\5JL6STI6\\2104.html:text/html},
}

@article{benjdira_unsupervised_2019,
	title = {Unsupervised {Domain} {Adaptation} using {Generative} {Adversarial} {Networks} for {Semantic} {Segmentation} of {Aerial} {Images}},
	volume = {11},
	issn = {2072-4292},
	url = {http://arxiv.org/abs/1905.03198},
	doi = {10.3390/rs11111369},
	abstract = {Segmenting aerial images is being of great potential in surveillance and scene understanding of urban areas. It provides a mean for automatic reporting of the different events that happen in inhabited areas. This remarkably promotes public safety and traffic management applications. After the wide adoption of convolutional neural networks methods, the accuracy of semantic segmentation algorithms could easily surpass 80\% if a robust dataset is provided. Despite this success, the deployment of a pre-trained segmentation model to survey a new city that is not included in the training set significantly decreases the accuracy. This is due to the domain shift between the source dataset on which the model is trained and the new target domain of the new city images. In this paper, we address this issue and consider the challenge of domain adaptation in semantic segmentation of aerial images. We design an algorithm that reduces the domain shift impact using Generative Adversarial Networks (GANs). In the experiments, we test the proposed methodology on the International Society for Photogrammetry and Remote Sensing (ISPRS) semantic segmentation dataset and found that our method improves the overall accuracy from 35\% to 52\% when passing from Potsdam domain (considered as source domain) to Vaihingen domain (considered as target domain). In addition, the method allows recovering efficiently the inverted classes due to sensor variation. In particular, it improves the average segmentation accuracy of the inverted classes due to sensor variation from 14\% to 61\%.},
	number = {11},
	urldate = {2022-06-09},
	journal = {Remote Sensing},
	author = {Benjdira, Bilel and Bazi, Yakoub and Koubaa, Anis and Ouni, Kais},
	month = jun,
	year = {2019},
	note = {arXiv:1905.03198 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {1369},
	annote = {Comment: submitted to a journal},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\8VXX435D\\Benjdira et al. - 2019 - Unsupervised Domain Adaptation using Generative Ad.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\2Z3Q7HS8\\1905.html:text/html},
}

@techreport{vu_advent_2019,
	title = {{ADVENT}: {Adversarial} {Entropy} {Minimization} for {Domain} {Adaptation} in {Semantic} {Segmentation}},
	shorttitle = {{ADVENT}},
	url = {http://arxiv.org/abs/1811.12833},
	abstract = {Semantic segmentation is a key problem for many computer vision tasks. While approaches based on convolutional neural networks constantly break new records on different benchmarks, generalizing well to diverse testing environments remains a major challenge. In numerous real world applications, there is indeed a large gap between data distributions in train and test domains, which results in severe performance loss at run-time. In this work, we address the task of unsupervised domain adaptation in semantic segmentation with losses based on the entropy of the pixel-wise predictions. To this end, we propose two novel, complementary methods using (i) entropy loss and (ii) adversarial loss respectively. We demonstrate state-of-the-art performance in semantic segmentation on two challenging "synthetic-2-real" set-ups and show that the approach can also be used for detection.},
	number = {arXiv:1811.12833},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Vu, Tuan-Hung and Jain, Himalaya and Bucher, Maxime and Cord, Matthieu and Pérez, Patrick},
	month = apr,
	year = {2019},
	doi = {10.48550/arXiv.1811.12833},
	note = {arXiv:1811.12833 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted in CVPR'19. Code is available at https://github.com/valeoai/ADVENT},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\HMISZ7QZ\\Vu et al. - 2019 - ADVENT Adversarial Entropy Minimization for Domai.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\Y2QIE4XV\\1811.html:text/html},
}

@techreport{li_bidirectional_2019,
	title = {Bidirectional {Learning} for {Domain} {Adaptation} of {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/1904.10620},
	abstract = {Domain adaptation for semantic image segmentation is very necessary since manually labeling large datasets with pixel-level labels is expensive and time consuming. Existing domain adaptation techniques either work on limited datasets, or yield not so good performance compared with supervised learning. In this paper, we propose a novel bidirectional learning framework for domain adaptation of segmentation. Using the bidirectional learning, the image translation model and the segmentation adaptation model can be learned alternatively and promote to each other. Furthermore, we propose a self-supervised learning algorithm to learn a better segmentation adaptation model and in return improve the image translation model. Experiments show that our method is superior to the state-of-the-art methods in domain adaptation of segmentation with a big margin. The source code is available at https://github.com/liyunsheng13/BDL.},
	number = {arXiv:1904.10620},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Li, Yunsheng and Yuan, Lu and Vasconcelos, Nuno},
	month = apr,
	year = {2019},
	doi = {10.48550/arXiv.1904.10620},
	note = {arXiv:1904.10620 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted by CVPR2019},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\JM5CPJ44\\Li et al. - 2019 - Bidirectional Learning for Domain Adaptation of Se.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\GINX7ZZU\\1904.html:text/html},
}

@techreport{wang_differential_2020,
	title = {Differential {Treatment} for {Stuff} and {Things}: {A} {Simple} {Unsupervised} {Domain} {Adaptation} {Method} for {Semantic} {Segmentation}},
	shorttitle = {Differential {Treatment} for {Stuff} and {Things}},
	url = {http://arxiv.org/abs/2003.08040},
	abstract = {We consider the problem of unsupervised domain adaptation for semantic segmentation by easing the domain shift between the source domain (synthetic data) and the target domain (real data) in this work. State-of-the-art approaches prove that performing semantic-level alignment is helpful in tackling the domain shift issue. Based on the observation that stuff categories usually share similar appearances across images of different domains while things (i.e. object instances) have much larger differences, we propose to improve the semantic-level alignment with different strategies for stuff regions and for things: 1) for the stuff categories, we generate feature representation for each class and conduct the alignment operation from the target domain to the source domain; 2) for the thing categories, we generate feature representation for each individual instance and encourage the instance in the target domain to align with the most similar one in the source domain. In this way, the individual differences within thing categories will also be considered to alleviate over-alignment. In addition to our proposed method, we further reveal the reason why the current adversarial loss is often unstable in minimizing the distribution discrepancy and show that our method can help ease this issue by minimizing the most similar stuff and instance features between the source and the target domains. We conduct extensive experiments in two unsupervised domain adaptation tasks, i.e. GTA5 to Cityscapes and SYNTHIA to Cityscapes, and achieve the new state-of-the-art segmentation accuracy.},
	number = {arXiv:2003.08040},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Wang, Zhonghao and Yu, Mo and Wei, Yunchao and Feris, Rogerio and Xiong, Jinjun and Hwu, Wen-mei and Huang, Thomas S. and Shi, Humphrey},
	month = jun,
	year = {2020},
	doi = {10.48550/arXiv.2003.08040},
	note = {arXiv:2003.08040 [cs, eess]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
	annote = {Comment: CVPR 2020},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\7SHNMK4B\\Wang et al. - 2020 - Differential Treatment for Stuff and Things A Sim.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\RPBUYYAL\\2003.html:text/html},
}

@techreport{wright_transformer_2020,
	title = {Transformer {Based} {Multi}-{Source} {Domain} {Adaptation}},
	url = {http://arxiv.org/abs/2009.07806},
	abstract = {In practical machine learning settings, the data on which a model must make predictions often come from a different distribution than the data it was trained on. Here, we investigate the problem of unsupervised multi-source domain adaptation, where a model is trained on labelled data from multiple source domains and must make predictions on a domain for which no labelled data has been seen. Prior work with CNNs and RNNs has demonstrated the benefit of mixture of experts, where the predictions of multiple domain expert classifiers are combined; as well as domain adversarial training, to induce a domain agnostic representation space. Inspired by this, we investigate how such methods can be effectively applied to large pretrained transformer models. We find that domain adversarial training has an effect on the learned representations of these models while having little effect on their performance, suggesting that large transformer-based models are already relatively robust across domains. Additionally, we show that mixture of experts leads to significant performance improvements by comparing several variants of mixing functions, including one novel mixture based on attention. Finally, we demonstrate that the predictions of large pretrained transformer based domain experts are highly homogenous, making it challenging to learn effective functions for mixing their predictions.},
	number = {arXiv:2009.07806},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Wright, Dustin and Augenstein, Isabelle},
	month = sep,
	year = {2020},
	note = {arXiv:2009.07806 [cs, stat]
type: article},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 12 pages, 3 figures, 5 tables},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\VHD99C4G\\Wright and Augenstein - 2020 - Transformer Based Multi-Source Domain Adaptation.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\5T543Z59\\2009.html:text/html},
}

@techreport{shakeri_end--end_2020,
	title = {End-to-{End} {Synthetic} {Data} {Generation} for {Domain} {Adaptation} of {Question} {Answering} {Systems}},
	url = {http://arxiv.org/abs/2010.06028},
	abstract = {We propose an end-to-end approach for synthetic QA data generation. Our model comprises a single transformer-based encoder-decoder network that is trained end-to-end to generate both answers and questions. In a nutshell, we feed a passage to the encoder and ask the decoder to generate a question and an answer token-by-token. The likelihood produced in the generation process is used as a filtering score, which avoids the need for a separate filtering model. Our generator is trained by fine-tuning a pretrained LM using maximum likelihood estimation. The experimental results indicate significant improvements in the domain adaptation of QA models outperforming current state-of-the-art methods.},
	number = {arXiv:2010.06028},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Shakeri, Siamak and Santos, Cicero Nogueira dos and Zhu, Henry and Ng, Patrick and Nan, Feng and Wang, Zhiguo and Nallapati, Ramesh and Xiang, Bing},
	month = oct,
	year = {2020},
	doi = {10.48550/arXiv.2010.06028},
	note = {arXiv:2010.06028 [cs]
type: article},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: EMNLP 2020},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\JV6ASG28\\Shakeri et al. - 2020 - End-to-End Synthetic Data Generation for Domain Ad.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\GBVEB3BX\\2010.html:text/html},
}

@techreport{liu_multi-task_2019,
	title = {Multi-{Task} {Deep} {Neural} {Networks} for {Natural} {Language} {Understanding}},
	url = {http://arxiv.org/abs/1901.11504},
	abstract = {In this paper, we present a Multi-Task Deep Neural Network (MT-DNN) for learning representations across multiple natural language understanding (NLU) tasks. MT-DNN not only leverages large amounts of cross-task data, but also benefits from a regularization effect that leads to more general representations in order to adapt to new tasks and domains. MT-DNN extends the model proposed in Liu et al. (2015) by incorporating a pre-trained bidirectional transformer language model, known as BERT (Devlin et al., 2018). MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI, SciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.7\% (2.2\% absolute improvement). We also demonstrate using the SNLI and SciTail datasets that the representations learned by MT-DNN allow domain adaptation with substantially fewer in-domain labels than the pre-trained BERT representations. The code and pre-trained models are publicly available at https://github.com/namisan/mt-dnn.},
	number = {arXiv:1901.11504},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Liu, Xiaodong and He, Pengcheng and Chen, Weizhu and Gao, Jianfeng},
	month = may,
	year = {2019},
	doi = {10.48550/arXiv.1901.11504},
	note = {arXiv:1901.11504 [cs]
type: article},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 10 pages, 2 figures and 5 tables; Accepted by ACL 2019},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\MZLKJKNN\\Liu et al. - 2019 - Multi-Task Deep Neural Networks for Natural Langua.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\GNUXTJSN\\1901.html:text/html},
}

@techreport{yang_transformer-based_2021,
	title = {Transformer-{Based} {Source}-{Free} {Domain} {Adaptation}},
	url = {http://arxiv.org/abs/2105.14138},
	abstract = {In this paper, we study the task of source-free domain adaptation (SFDA), where the source data are not available during target adaptation. Previous works on SFDA mainly focus on aligning the cross-domain distributions. However, they ignore the generalization ability of the pretrained source model, which largely influences the initial target outputs that are vital to the target adaptation stage. To address this, we make the interesting observation that the model accuracy is highly correlated with whether or not attention is focused on the objects in an image. To this end, we propose a generic and effective framework based on Transformer, named TransDA, for learning a generalized model for SFDA. Specifically, we apply the Transformer as the attention module and inject it into a convolutional network. By doing so, the model is encouraged to turn attention towards the object regions, which can effectively improve the model's generalization ability on the target domains. Moreover, a novel self-supervised knowledge distillation approach is proposed to adapt the Transformer with target pseudo-labels, thus further encouraging the network to focus on the object regions. Experiments on three domain adaptation tasks, including closed-set, partial-set, and open-set adaption, demonstrate that TransDA can greatly improve the adaptation accuracy and produce state-of-the-art results. The source code and trained models are available at https://github.com/ygjwd12345/TransDA.},
	number = {arXiv:2105.14138},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Yang, Guanglei and Tang, Hao and Zhong, Zhun and Ding, Mingli and Shao, Ling and Sebe, Nicu and Ricci, Elisa},
	month = may,
	year = {2021},
	doi = {10.48550/arXiv.2105.14138},
	note = {arXiv:2105.14138 [cs]
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\BDQLWTI8\\Yang et al. - 2021 - Transformer-Based Source-Free Domain Adaptation.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\8I35ASIW\\2105.html:text/html},
}

@article{chapelle_semi-supervised_2005,
	title = {Semi-{Supervised} {Classiﬁcation} by {Low} {Density} {Separation}},
	abstract = {We believe that the cluster assumption is key to successful semi-supervised learning. Based on this, we propose three semi-supervised algorithms: 1. deriving graph-based distances that emphazise low density regions between clusters, followed by training a standard SVM; 2. optimizing the Transductive SVM objective function, which places the decision boundary in low density regions, by gradient descent; 3. combining the ﬁrst two to make maximum use of the cluster assumption. We compare with state of the art algorithms and demonstrate superior accuracy for the latter two methods.},
	language = {en},
	author = {Chapelle, Olivier and Zien, Alexander},
	year = {2005},
	pages = {8},
	file = {Chapelle and Zien - Semi-Supervised Classiﬁcation by Low Density Separ.pdf:C\:\\Users\\morit\\Zotero\\storage\\VPTNK8P5\\Chapelle and Zien - Semi-Supervised Classiﬁcation by Low Density Separ.pdf:application/pdf},
}

@inproceedings{wang_exploring_2021,
	title = {Exploring {Sequence} {Feature} {Alignment} for {Domain} {Adaptive} {Detection} {Transformers}},
	url = {http://arxiv.org/abs/2107.12636},
	doi = {10.1145/3474085.3475317},
	abstract = {Detection transformers have recently shown promising object detection results and attracted increasing attention. However, how to develop effective domain adaptation techniques to improve its cross-domain performance remains unexplored and unclear. In this paper, we delve into this topic and empirically find that direct feature distribution alignment on the CNN backbone only brings limited improvements, as it does not guarantee domain-invariant sequence features in the transformer for prediction. To address this issue, we propose a novel Sequence Feature Alignment (SFA) method that is specially designed for the adaptation of detection transformers. Technically, SFA consists of a domain query-based feature alignment (DQFA) module and a token-wise feature alignment (TDA) module. In DQFA, a novel domain query is used to aggregate and align global context from the token sequence of both domains. DQFA reduces the domain discrepancy in global feature representations and object relations when deploying in the transformer encoder and decoder, respectively. Meanwhile, TDA aligns token features in the sequence from both domains, which reduces the domain gaps in local and instance-level feature representations in the transformer encoder and decoder, respectively. Besides, a novel bipartite matching consistency loss is proposed to enhance the feature discriminability for robust object detection. Experiments on three challenging benchmarks show that SFA outperforms state-of-the-art domain adaptive object detection methods. Code has been made available at: https://github.com/encounter1997/SFA.},
	urldate = {2022-06-09},
	booktitle = {Proceedings of the 29th {ACM} {International} {Conference} on {Multimedia}},
	author = {Wang, Wen and Cao, Yang and Zhang, Jing and He, Fengxiang and Zha, Zheng-Jun and Wen, Yonggang and Tao, Dacheng},
	month = oct,
	year = {2021},
	note = {arXiv:2107.12636 [cs]},
	keywords = {68T45 (Primary) 68T07 (Secondary), Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, I.2.10, I.4.9},
	pages = {1730--1738},
	annote = {Comment: Accepted by ACM MM2021. Source code is available at: https://github.com/encounter1997/SFA. Update acknowledgment},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\YNRIF2S6\\Wang et al. - 2021 - Exploring Sequence Feature Alignment for Domain Ad.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\C227KX8A\\2107.html:text/html},
}

@techreport{li_trear_2021,
	title = {Trear: {Transformer}-based {RGB}-{D} {Egocentric} {Action} {Recognition}},
	shorttitle = {Trear},
	url = {http://arxiv.org/abs/2101.03904},
	abstract = {In this paper, we propose a {\textbackslash}textbf\{Tr\}ansformer-based RGB-D {\textbackslash}textbf\{e\}gocentric {\textbackslash}textbf\{a\}ction {\textbackslash}textbf\{r\}ecognition framework, called Trear. It consists of two modules, inter-frame attention encoder and mutual-attentional fusion block. Instead of using optical flow or recurrent units, we adopt self-attention mechanism to model the temporal structure of the data from different modalities. Input frames are cropped randomly to mitigate the effect of the data redundancy. Features from each modality are interacted through the proposed fusion block and combined through a simple yet effective fusion operation to produce a joint RGB-D representation. Empirical experiments on two large egocentric RGB-D datasets, THU-READ and FPHA, and one small dataset, WCVS, have shown that the proposed method outperforms the state-of-the-art results by a large margin.},
	number = {arXiv:2101.03904},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Li, Xiangyu and Hou, Yonghong and Wang, Pichao and Gao, Zhimin and Xu, Mingliang and Li, Wanqing},
	month = jan,
	year = {2021},
	doi = {10.48550/arXiv.2101.03904},
	note = {arXiv:2101.03904 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted by IEEE Transactions},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\ATKINBS3\\Li et al. - 2021 - Trear Transformer-based RGB-D Egocentric Action R.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\WD3FVBKZ\\2101.html:text/html},
}

@techreport{tsai_multimodal_2019,
	title = {Multimodal {Transformer} for {Unaligned} {Multimodal} {Language} {Sequences}},
	url = {http://arxiv.org/abs/1906.00295},
	abstract = {Human language is often multimodal, which comprehends a mixture of natural language, facial gestures, and acoustic behaviors. However, two major challenges in modeling such multimodal human language time-series data exist: 1) inherent data non-alignment due to variable sampling rates for the sequences from each modality; and 2) long-range dependencies between elements across modalities. In this paper, we introduce the Multimodal Transformer (MulT) to generically address the above issues in an end-to-end manner without explicitly aligning the data. At the heart of our model is the directional pairwise crossmodal attention, which attends to interactions between multimodal sequences across distinct time steps and latently adapt streams from one modality to another. Comprehensive experiments on both aligned and non-aligned multimodal time-series show that our model outperforms state-of-the-art methods by a large margin. In addition, empirical analysis suggests that correlated crossmodal signals are able to be captured by the proposed crossmodal attention mechanism in MulT.},
	number = {arXiv:1906.00295},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Tsai, Yao-Hung Hubert and Bai, Shaojie and Liang, Paul Pu and Kolter, J. Zico and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
	month = jun,
	year = {2019},
	note = {arXiv:1906.00295 [cs]
type: article},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\EY3PY4UU\\Tsai et al. - 2019 - Multimodal Transformer for Unaligned Multimodal La.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\8GRG2VGU\\1906.html:text/html},
}

@techreport{sun_return_2015,
	title = {Return of {Frustratingly} {Easy} {Domain} {Adaptation}},
	url = {http://arxiv.org/abs/1511.05547},
	abstract = {Unlike human learning, machine learning often fails to handle changes between training (source) and test (target) input distributions. Such domain shifts, common in practical scenarios, severely damage the performance of conventional machine learning methods. Supervised domain adaptation methods have been proposed for the case when the target data have labels, including some that perform very well despite being "frustratingly easy" to implement. However, in practice, the target domain is often unlabeled, requiring unsupervised adaptation. We propose a simple, effective, and efficient method for unsupervised domain adaptation called CORrelation ALignment (CORAL). CORAL minimizes domain shift by aligning the second-order statistics of source and target distributions, without requiring any target labels. Even though it is extraordinarily simple--it can be implemented in four lines of Matlab code--CORAL performs remarkably well in extensive evaluations on standard benchmark datasets.},
	number = {arXiv:1511.05547},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Sun, Baochen and Feng, Jiashi and Saenko, Kate},
	month = dec,
	year = {2015},
	doi = {10.48550/arXiv.1511.05547},
	note = {arXiv:1511.05547 [cs]
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: Fixed typos. Full paper to appear in AAAI-16. Extended Abstract of the full paper to appear in TASK-CV 2015 workshop},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\2V8S49GY\\Sun et al. - 2015 - Return of Frustratingly Easy Domain Adaptation.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\Z62CPQZP\\1511.html:text/html},
}

@inproceedings{gretton_kernel_2006,
	title = {A {Kernel} {Method} for the {Two}-{Sample}-{Problem}},
	volume = {19},
	url = {https://proceedings.neurips.cc/paper/2006/hash/e9fb2eda3d9c55a0d89c98d6c54b5b3e-Abstract.html},
	abstract = {We propose two statistical tests to determine if two samples are from different dis- tributions. Our test statistic is in both cases the distance between the means of the two samples mapped into a reproducing kernel Hilbert space (RKHS). The ﬁrst test is based on a large deviation bound for the test statistic, while the second is based on the asymptotic distribution of this statistic. The test statistic can be com- puted in O(m2) time. We apply our approach to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where our test performs strongly. We also demonstrate excellent performance when compar- ing distributions over graphs, for which no alternative tests currently exist.},
	urldate = {2022-06-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Gretton, Arthur and Borgwardt, Karsten and Rasch, Malte and Schölkopf, Bernhard and Smola, Alex},
	year = {2006},
	file = {Full Text PDF:C\:\\Users\\morit\\Zotero\\storage\\6G9TNHFP\\Gretton et al. - 2006 - A Kernel Method for the Two-Sample-Problem.pdf:application/pdf},
}

@techreport{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	number = {arXiv:1406.2661},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	month = jun,
	year = {2014},
	doi = {10.48550/arXiv.1406.2661},
	note = {arXiv:1406.2661 [cs, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\UBZEPSY7\\Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\DHIB6KSB\\1406.html:text/html},
}

@article{larochelle_zero-data_2008,
	title = {Zero-data {Learning} of {New} {Tasks}},
	abstract = {We introduce the problem of zero-data learning, where a model must generalize to classes or tasks for which no training data are available and only a description of the classes or tasks are provided. Zero-data learning is useful for problems where the set of classes to distinguish or tasks to solve is very large and is not entirely covered by the training data. The main contributions of this work lie in the presentation of a general formalization of zero-data learning, in an experimental analysis of its properties and in empirical evidence showing that generalization is possible and signiﬁcant in this context. The experimental work of this paper addresses two classiﬁcation problems of character recognition and a multitask ranking problem in the context of drug discovery. Finally, we conclude by discussing how this new framework could lead to a novel perspective on how to extend machine learning towards AI, where an agent can be given a speciﬁcation for a learning problem before attempting to solve it (with very few or even zero examples).},
	language = {en},
	author = {Larochelle, Hugo and Erhan, Dumitru and Bengio, Yoshua},
	year = {2008},
	pages = {6},
	file = {Larochelle et al. - Zero-data Learning of New Tasks.pdf:C\:\\Users\\morit\\Zotero\\storage\\SCGG365L\\Larochelle et al. - Zero-data Learning of New Tasks.pdf:application/pdf},
}

@techreport{richter_playing_2016,
	title = {Playing for {Data}: {Ground} {Truth} from {Computer} {Games}},
	shorttitle = {Playing for {Data}},
	url = {http://arxiv.org/abs/1608.02192},
	abstract = {Recent progress in computer vision has been driven by high-capacity models trained on large datasets. Unfortunately, creating large datasets with pixel-level labels has been extremely costly due to the amount of human effort required. In this paper, we present an approach to rapidly creating pixel-accurate semantic label maps for images extracted from modern computer games. Although the source code and the internal operation of commercial games are inaccessible, we show that associations between image patches can be reconstructed from the communication between the game and the graphics hardware. This enables rapid propagation of semantic labels within and across images synthesized by the game, with no access to the source code or the content. We validate the presented approach by producing dense pixel-level semantic annotations for 25 thousand images synthesized by a photorealistic open-world computer game. Experiments on semantic segmentation datasets show that using the acquired data to supplement real-world images significantly increases accuracy and that the acquired data enables reducing the amount of hand-labeled real-world data: models trained with game data and just 1/3 of the CamVid training set outperform models trained on the complete CamVid training set.},
	number = {arXiv:1608.02192},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Richter, Stephan R. and Vineet, Vibhav and Roth, Stefan and Koltun, Vladlen},
	month = aug,
	year = {2016},
	doi = {10.48550/arXiv.1608.02192},
	note = {arXiv:1608.02192 [cs]
version: 1
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, I.4.8},
	annote = {Comment: Accepted to the 14th European Conference on Computer Vision (ECCV 2016)},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\TH397S2X\\Richter et al. - 2016 - Playing for Data Ground Truth from Computer Games.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\WYVRY5WL\\1608.html:text/html},
}

@inproceedings{ros_synthia_2016,
	address = {Las Vegas, NV, USA},
	title = {The {SYNTHIA} {Dataset}: {A} {Large} {Collection} of {Synthetic} {Images} for {Semantic} {Segmentation} of {Urban} {Scenes}},
	isbn = {978-1-4673-8851-1},
	shorttitle = {The {SYNTHIA} {Dataset}},
	url = {http://ieeexplore.ieee.org/document/7780721/},
	doi = {10.1109/CVPR.2016.352},
	abstract = {Vision-based semantic segmentation in urban scenarios is a key functionality for autonomous driving. Recent revolutionary results of deep convolutional neural networks (DCNNs) foreshadow the advent of reliable classiﬁers to perform such visual tasks. However, DCNNs require learning of many parameters from raw images; thus, having a sufﬁcient amount of diverse images with class annotations is needed. These annotations are obtained via cumbersome, human labour which is particularly challenging for semantic segmentation since pixel-level annotations are required. In this paper, we propose to use a virtual world to automatically generate realistic synthetic images with pixel-level annotations. Then, we address the question of how useful such data can be for semantic segmentation – in particular, when using a DCNN paradigm. In order to answer this question we have generated a synthetic collection of diverse urban images, named SYNTHIA, with automatically generated class annotations. We use SYNTHIA in combination with publicly available real-world urban images with manually provided annotations. Then, we conduct experiments with DCNNs that show how the inclusion of SYNTHIA in the training stage signiﬁcantly improves performance on the semantic segmentation task.},
	language = {en},
	urldate = {2022-06-09},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Ros, German and Sellart, Laura and Materzynska, Joanna and Vazquez, David and Lopez, Antonio M.},
	month = jun,
	year = {2016},
	pages = {3234--3243},
	file = {Ros et al. - 2016 - The SYNTHIA Dataset A Large Collection of Synthet.pdf:C\:\\Users\\morit\\Zotero\\storage\\RC2ETRSF\\Ros et al. - 2016 - The SYNTHIA Dataset A Large Collection of Synthet.pdf:application/pdf},
}

@inproceedings{bermudez-chacon_domain-adaptive_2018,
	title = {A domain-adaptive two-stream {U}-{Net} for electron microscopy image segmentation},
	doi = {10.1109/ISBI.2018.8363602},
	abstract = {Deep networks such as the U-Net are outstanding at segmenting biomedical images when enough training data is available, but only then. Here we introduce a Domain Adaptation approach that relies on two coupled U-Nets that either regularize or share corresponding weights between the two streams, along with a differentiable loss function that approximates the Jaccard index, to leverage training data from one domain in which it is plentiful, to adapt the network weights in another where it is scarce. We showcase our approach for the purpose of segmenting mitochondria and synapses from electron microscopy image stacks of mouse brain, when we have enough training data for one brain region but only very little for another. In such cases, we outperform state-of-the-art Domain Adaptation methods.},
	booktitle = {2018 {IEEE} 15th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI} 2018)},
	author = {Bermúdez-Chacón, Róger and Márquez-Neila, Pablo and Salzmann, Mathieu and Fua, Pascal},
	month = apr,
	year = {2018},
	note = {ISSN: 1945-8452},
	keywords = {Domain Adaptation, Electron Microscopy, Image segmentation, Indexes, Machine Learning, Mice, Standards, Synapses, Training, Training data},
	pages = {400--404},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\morit\\Zotero\\storage\\7K39LXV2\\8363602.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\morit\\Zotero\\storage\\NEJMPCA6\\Bermúdez-Chacón et al. - 2018 - A domain-adaptive two-stream U-Net for electron mi.pdf:application/pdf},
}

@techreport{lin_refinenet_2016,
	title = {{RefineNet}: {Multi}-{Path} {Refinement} {Networks} for {High}-{Resolution} {Semantic} {Segmentation}},
	shorttitle = {{RefineNet}},
	url = {http://arxiv.org/abs/1611.06612},
	abstract = {Recently, very deep convolutional neural networks (CNNs) have shown outstanding performance in object recognition and have also been the first choice for dense classification problems such as semantic segmentation. However, repeated subsampling operations like pooling or convolution striding in deep CNNs lead to a significant decrease in the initial image resolution. Here, we present RefineNet, a generic multi-path refinement network that explicitly exploits all the information available along the down-sampling process to enable high-resolution prediction using long-range residual connections. In this way, the deeper layers that capture high-level semantic features can be directly refined using fine-grained features from earlier convolutions. The individual components of RefineNet employ residual connections following the identity mapping mindset, which allows for effective end-to-end training. Further, we introduce chained residual pooling, which captures rich background context in an efficient manner. We carry out comprehensive experiments and set new state-of-the-art results on seven public datasets. In particular, we achieve an intersection-over-union score of 83.4 on the challenging PASCAL VOC 2012 dataset, which is the best reported result to date.},
	number = {arXiv:1611.06612},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Lin, Guosheng and Milan, Anton and Shen, Chunhua and Reid, Ian},
	month = nov,
	year = {2016},
	doi = {10.48550/arXiv.1611.06612},
	note = {arXiv:1611.06612 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\45CJJVN3\\Lin et al. - 2016 - RefineNet Multi-Path Refinement Networks for High.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\UKQMIQYT\\1611.html:text/html},
}

@techreport{hoffman_cycada_2017,
	title = {{CyCADA}: {Cycle}-{Consistent} {Adversarial} {Domain} {Adaptation}},
	shorttitle = {{CyCADA}},
	url = {http://arxiv.org/abs/1711.03213},
	abstract = {Domain adaptation is critical for success in new, unseen environments. Adversarial adaptation models applied in feature spaces discover domain invariant representations, but are difficult to visualize and sometimes fail to capture pixel-level and low-level domain shifts. Recent work has shown that generative adversarial networks combined with cycle-consistency constraints are surprisingly effective at mapping images between domains, even without the use of aligned image pairs. We propose a novel discriminatively-trained Cycle-Consistent Adversarial Domain Adaptation model. CyCADA adapts representations at both the pixel-level and feature-level, enforces cycle-consistency while leveraging a task loss, and does not require aligned pairs. Our model can be applied in a variety of visual recognition and prediction settings. We show new state-of-the-art results across multiple adaptation tasks, including digit classification and semantic segmentation of road scenes demonstrating transfer from synthetic to real world domains.},
	number = {arXiv:1711.03213},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Hoffman, Judy and Tzeng, Eric and Park, Taesung and Zhu, Jun-Yan and Isola, Phillip and Saenko, Kate and Efros, Alexei A. and Darrell, Trevor},
	month = dec,
	year = {2017},
	doi = {10.48550/arXiv.1711.03213},
	note = {arXiv:1711.03213 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\LV3BV7WQ\\Hoffman et al. - 2017 - CyCADA Cycle-Consistent Adversarial Domain Adapta.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\LITDW7TI\\1711.html:text/html},
}

@article{lin_maritime_2017,
	title = {Maritime {Semantic} {Labeling} of {Optical} {Remote} {Sensing} {Images} with {Multi}-{Scale} {Fully} {Convolutional} {Network}},
	volume = {9},
	doi = {10.3390/rs9050480},
	abstract = {In current remote sensing literature, the problems of sea-land segmentation and ship detection (including in-dock ships) are investigated separately despite the high correlation between them. This inhibits joint optimization and makes the implementation of the methods highly complicated. In this paper, we propose a novel fully convolutional network to accomplish the two tasks simultaneously, in a semantic labeling fashion, i.e., to label every pixel of the image into 3 classes, sea, land and ships. A multi-scale structure for the network is proposed to address the huge scale gap between different classes of targets, i.e., sea/land and ships. Conventional multi-scale structure utilizes shortcuts to connect low level, fine scale feature maps to high level ones to increase the network’s ability to produce finer results. In contrast, our proposed multi-scale structure focuses on increasing the receptive field of the network while maintaining the ability towards fine scale details. The multi-scale convolution network accommodates the huge scale difference between sea-land and ships and provides comprehensive features, and is able to accomplish the tasks in an end-to-end manner that is easy for implementation and feasible for joint optimization. In the network, the input forks into fine-scale and coarse-scale paths, which share the same convolution layers to minimize network parameter increase, and then are joined together to produce the final result. The experiments show that the network tackles the semantic labeling problem with improved performance.},
	journal = {Remote Sensing},
	author = {Lin, Haoning and Shi, Zhenwei and Zou, Zhengxia},
	month = may,
	year = {2017},
	pages = {480},
	file = {Full Text:C\:\\Users\\morit\\Zotero\\storage\\I4TH6T3Y\\Lin et al. - 2017 - Maritime Semantic Labeling of Optical Remote Sensi.pdf:application/pdf},
}

@techreport{yu_bisenet_2018,
	title = {{BiSeNet}: {Bilateral} {Segmentation} {Network} for {Real}-time {Semantic} {Segmentation}},
	shorttitle = {{BiSeNet}},
	url = {http://arxiv.org/abs/1808.00897},
	abstract = {Semantic segmentation requires both rich spatial information and sizeable receptive field. However, modern approaches usually compromise spatial resolution to achieve real-time inference speed, which leads to poor performance. In this paper, we address this dilemma with a novel Bilateral Segmentation Network (BiSeNet). We first design a Spatial Path with a small stride to preserve the spatial information and generate high-resolution features. Meanwhile, a Context Path with a fast downsampling strategy is employed to obtain sufficient receptive field. On top of the two paths, we introduce a new Feature Fusion Module to combine features efficiently. The proposed architecture makes a right balance between the speed and segmentation performance on Cityscapes, CamVid, and COCO-Stuff datasets. Specifically, for a 2048x1024 input, we achieve 68.4\% Mean IOU on the Cityscapes test dataset with speed of 105 FPS on one NVIDIA Titan XP card, which is significantly faster than the existing methods with comparable performance.},
	number = {arXiv:1808.00897},
	urldate = {2022-06-10},
	institution = {arXiv},
	author = {Yu, Changqian and Wang, Jingbo and Peng, Chao and Gao, Changxin and Yu, Gang and Sang, Nong},
	month = aug,
	year = {2018},
	doi = {10.48550/arXiv.1808.00897},
	note = {arXiv:1808.00897 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted to ECCV 2018. 17 pages, 4 figures, 9 tables},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\CCSKYE8Z\\Yu et al. - 2018 - BiSeNet Bilateral Segmentation Network for Real-t.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\PY4DR7VG\\1808.html:text/html},
}

@techreport{karras_progressive_2018,
	title = {Progressive {Growing} of {GANs} for {Improved} {Quality}, {Stability}, and {Variation}},
	url = {http://arxiv.org/abs/1710.10196},
	abstract = {We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024{\textasciicircum}2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.},
	number = {arXiv:1710.10196},
	urldate = {2022-06-10},
	institution = {arXiv},
	author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
	month = feb,
	year = {2018},
	doi = {10.48550/arXiv.1710.10196},
	note = {arXiv:1710.10196 [cs, stat]
version: 3
type: article},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: Final ICLR 2018 version},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\A6EB86J6\\Karras et al. - 2018 - Progressive Growing of GANs for Improved Quality, .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\ZRJZ3CMR\\1710.html:text/html},
}

@techreport{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	number = {arXiv:2005.14165},
	urldate = {2022-06-10},
	institution = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	doi = {10.48550/arXiv.2005.14165},
	note = {arXiv:2005.14165 [cs]
type: article},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 40+32 pages},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\8VZSFX7Q\\Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\QJV3FZAG\\2005.html:text/html},
}

@inproceedings{saenko_adapting_2010,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Adapting {Visual} {Category} {Models} to {New} {Domains}},
	isbn = {978-3-642-15561-1},
	doi = {10.1007/978-3-642-15561-1_16},
	abstract = {Domain adaptation is an important emerging topic in computer vision. In this paper, we present one of the first studies of domain shift in the context of object recognition. We introduce a method that adapts object models acquired in a particular visual domain to new imaging conditions by learning a transformation that minimizes the effect of domain-induced changes in the feature distribution. The transformation is learned in a supervised manner and can be applied to categories for which there are no labeled examples in the new domain. While we focus our evaluation on object recognition tasks, the transform-based adaptation technique we develop is general and could be applied to non-image data. Another contribution is a new multi-domain object database, freely available for download. We experimentally demonstrate the ability of our method to improve recognition on categories with few or no target domain labels and moderate to large changes in the imaging conditions.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2010},
	publisher = {Springer},
	author = {Saenko, Kate and Kulis, Brian and Fritz, Mario and Darrell, Trevor},
	editor = {Daniilidis, Kostas and Maragos, Petros and Paragios, Nikos},
	year = {2010},
	keywords = {Domain Adaptation, Source Domain, Support Vector Machine, Target Domain, Visual Domain},
	pages = {213--226},
	file = {Full Text PDF:C\:\\Users\\morit\\Zotero\\storage\\VCZCRIYM\\Saenko et al. - 2010 - Adapting Visual Category Models to New Domains.pdf:application/pdf},
}

@techreport{ronneberger_u-net_2015,
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	shorttitle = {U-{Net}},
	url = {http://arxiv.org/abs/1505.04597},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	number = {arXiv:1505.04597},
	urldate = {2022-06-10},
	institution = {arXiv},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	month = may,
	year = {2015},
	doi = {10.48550/arXiv.1505.04597},
	note = {arXiv:1505.04597 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: conditionally accepted at MICCAI 2015},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\MUUDGJR6\\Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\5LEY68HC\\1505.html:text/html},
}

@article{geiger_vision_2013,
	title = {Vision meets robotics: {The} {KITTI} dataset},
	volume = {32},
	issn = {0278-3649, 1741-3176},
	shorttitle = {Vision meets robotics},
	url = {http://journals.sagepub.com/doi/10.1177/0278364913491297},
	doi = {10.1177/0278364913491297},
	abstract = {We present a novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving research. In total, we recorded 6 hours of trafﬁc scenarios at 10-100 Hz using a variety of sensor modalities such as highresolution color and grayscale stereo cameras, a Velodyne 3D laser scanner and a high-precision GPS/IMU inertial navigation system. The scenarios are diverse, capturing real-world trafﬁc situations and range from freeways over rural areas to innercity scenes with many static and dynamic objects. Our data is calibrated, synchronized and timestamped, and we provide the rectiﬁed and raw image sequences. Our dataset also contains object labels in the form of 3D tracklets and we provide online benchmarks for stereo, optical ﬂow, object detection and other tasks. This paper describes our recording platform, the data format and the utilities that we provide.},
	language = {en},
	number = {11},
	urldate = {2022-06-10},
	journal = {The International Journal of Robotics Research},
	author = {Geiger, A and Lenz, P and Stiller, C and Urtasun, R},
	month = sep,
	year = {2013},
	pages = {1231--1237},
	file = {Geiger2013IJRR.pdf:C\:\\Users\\morit\\Zotero\\storage\\HVYV5YB9\\Geiger2013IJRR.pdf:application/pdf},
}

@techreport{zhang_topformer_2022,
	title = {{TopFormer}: {Token} {Pyramid} {Transformer} for {Mobile} {Semantic} {Segmentation}},
	shorttitle = {{TopFormer}},
	url = {http://arxiv.org/abs/2204.05525},
	abstract = {Although vision transformers (ViTs) have achieved great success in computer vision, the heavy computational cost hampers their applications to dense prediction tasks such as semantic segmentation on mobile devices. In this paper, we present a mobile-friendly architecture named {\textbackslash}textbf\{To\}ken {\textbackslash}textbf\{P\}yramid Vision Trans{\textbackslash}textbf\{former\} ({\textbackslash}textbf\{TopFormer\}). The proposed {\textbackslash}textbf\{TopFormer\} takes Tokens from various scales as input to produce scale-aware semantic features, which are then injected into the corresponding tokens to augment the representation. Experimental results demonstrate that our method significantly outperforms CNN- and ViT-based networks across several semantic segmentation datasets and achieves a good trade-off between accuracy and latency. On the ADE20K dataset, TopFormer achieves 5{\textbackslash}\% higher accuracy in mIoU than MobileNetV3 with lower latency on an ARM-based mobile device. Furthermore, the tiny version of TopFormer achieves real-time inference on an ARM-based mobile device with competitive results. The code and models are available at: https://github.com/hustvl/TopFormer},
	number = {arXiv:2204.05525},
	urldate = {2022-06-10},
	institution = {arXiv},
	author = {Zhang, Wenqiang and Huang, Zilong and Luo, Guozhong and Chen, Tao and Wang, Xinggang and Liu, Wenyu and Yu, Gang and Shen, Chunhua},
	month = apr,
	year = {2022},
	note = {arXiv:2204.05525 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: To Appear at CVPR 2022},
	file = {arXiv Fulltext PDF:C\:\\Users\\morit\\Zotero\\storage\\BCU7QTJX\\Zhang et al. - 2022 - TopFormer Token Pyramid Transformer for Mobile Se.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\morit\\Zotero\\storage\\2UT47CJ7\\2204.html:text/html},
}
